Experiment: Fine-tuning LoRA
Timestamp: 20260219_155038
============================================================


============================================================
CONFIG
============================================================

Model: mistralai/Mistral-7B-Instruct-v0.2
Train file: data/data/train.jsonl
Val file: data/data/val.jsonl
Loaded 1264 training samples
Loaded 270 validation samples
Tokenization completed

============================================================
TRAINABLE PARAMETERS
============================================================


============================================================
TRAINING START
============================================================

Step 1 | loss: 2.9574477672576904
Step 1 | grad_norm: 3.3882551193237305
Step 1 | learning_rate: 0.0001
Step 1 | epoch: 0.0031645569620253164
Step 2 | loss: 2.7697806358337402
Step 2 | grad_norm: 2.9032399654388428
Step 2 | learning_rate: 9.989451476793249e-05
Step 2 | epoch: 0.006329113924050633
Step 3 | loss: 2.7895054817199707
Step 3 | grad_norm: 2.3779842853546143
Step 3 | learning_rate: 9.978902953586498e-05
Step 3 | epoch: 0.00949367088607595
Step 4 | loss: 2.2185654640197754
Step 4 | grad_norm: 2.166435718536377
Step 4 | learning_rate: 9.968354430379747e-05
Step 4 | epoch: 0.012658227848101266
Step 5 | loss: 1.8716541528701782
Step 5 | grad_norm: 1.6153186559677124
Step 5 | learning_rate: 9.957805907172997e-05
Step 5 | epoch: 0.015822784810126583
Step 6 | loss: 1.604727864265442
Step 6 | grad_norm: 1.4633501768112183
Step 6 | learning_rate: 9.947257383966246e-05
Step 6 | epoch: 0.0189873417721519
Step 7 | loss: 1.7110391855239868
Step 7 | grad_norm: 1.5541740655899048
Step 7 | learning_rate: 9.936708860759493e-05
Step 7 | epoch: 0.022151898734177215
Step 8 | loss: 1.495940923690796
Step 8 | grad_norm: 1.7573779821395874
Step 8 | learning_rate: 9.926160337552744e-05
Step 8 | epoch: 0.02531645569620253
Step 9 | loss: 1.3479139804840088
Step 9 | grad_norm: 1.8249917030334473
Step 9 | learning_rate: 9.915611814345992e-05
Step 9 | epoch: 0.028481012658227847
Step 10 | loss: 1.5015017986297607
Step 10 | grad_norm: 1.5819759368896484
Step 10 | learning_rate: 9.90506329113924e-05
Step 10 | epoch: 0.03164556962025317
Step 11 | loss: 1.028943419456482
Step 11 | grad_norm: 1.491581678390503
Step 11 | learning_rate: 9.89451476793249e-05
Step 11 | epoch: 0.03481012658227848
Step 12 | loss: 1.029482126235962
Step 12 | grad_norm: 1.524456262588501
Step 12 | learning_rate: 9.883966244725739e-05
Step 12 | epoch: 0.0379746835443038
Step 13 | loss: 0.7445499897003174
Step 13 | grad_norm: 0.9596006274223328
Step 13 | learning_rate: 9.873417721518988e-05
Step 13 | epoch: 0.04113924050632911
Step 14 | loss: 0.9878460168838501
Step 14 | grad_norm: 0.9905630350112915
Step 14 | learning_rate: 9.862869198312236e-05
Step 14 | epoch: 0.04430379746835443
Step 15 | loss: 0.9528625011444092
Step 15 | grad_norm: 1.1602674722671509
Step 15 | learning_rate: 9.852320675105485e-05
Step 15 | epoch: 0.04746835443037975
Step 16 | loss: 0.8438043594360352
Step 16 | grad_norm: 1.132194995880127
Step 16 | learning_rate: 9.841772151898735e-05
Step 16 | epoch: 0.05063291139240506
Step 17 | loss: 0.7585445642471313
Step 17 | grad_norm: 0.8488485813140869
Step 17 | learning_rate: 9.831223628691983e-05
Step 17 | epoch: 0.05379746835443038
Step 18 | loss: 0.936919093132019
Step 18 | grad_norm: 1.2115200757980347
Step 18 | learning_rate: 9.820675105485233e-05
Step 18 | epoch: 0.056962025316455694
Step 19 | loss: 1.0019961595535278
Step 19 | grad_norm: 1.1597176790237427
Step 19 | learning_rate: 9.810126582278482e-05
Step 19 | epoch: 0.060126582278481014
Step 20 | loss: 0.9509310722351074
Step 20 | grad_norm: 1.6610082387924194
Step 20 | learning_rate: 9.799578059071731e-05
Step 20 | epoch: 0.06329113924050633
Step 21 | loss: 0.6369742751121521
Step 21 | grad_norm: 0.9498901963233948
Step 21 | learning_rate: 9.78902953586498e-05
Step 21 | epoch: 0.06645569620253164
Step 22 | loss: 1.022803783416748
Step 22 | grad_norm: 1.2222089767456055
Step 22 | learning_rate: 9.778481012658228e-05
Step 22 | epoch: 0.06962025316455696
Step 23 | loss: 0.9220552444458008
Step 23 | grad_norm: 1.3286925554275513
Step 23 | learning_rate: 9.767932489451477e-05
Step 23 | epoch: 0.07278481012658228
Step 24 | loss: 0.9095754623413086
Step 24 | grad_norm: 0.9150605201721191
Step 24 | learning_rate: 9.757383966244726e-05
Step 24 | epoch: 0.0759493670886076
Step 25 | loss: 0.7773422598838806
Step 25 | grad_norm: 1.1382673978805542
Step 25 | learning_rate: 9.746835443037975e-05
Step 25 | epoch: 0.07911392405063292
Step 26 | loss: 0.7188123464584351
Step 26 | grad_norm: 0.8957614302635193
Step 26 | learning_rate: 9.736286919831225e-05
Step 26 | epoch: 0.08227848101265822
Step 27 | loss: 0.9817759990692139
Step 27 | grad_norm: 1.0913188457489014
Step 27 | learning_rate: 9.725738396624473e-05
Step 27 | epoch: 0.08544303797468354
Step 28 | loss: 0.8523227572441101
Step 28 | grad_norm: 1.019728660583496
Step 28 | learning_rate: 9.715189873417721e-05
Step 28 | epoch: 0.08860759493670886
Step 29 | loss: 0.7986142635345459
Step 29 | grad_norm: 1.169987678527832
Step 29 | learning_rate: 9.704641350210972e-05
Step 29 | epoch: 0.09177215189873418
Step 30 | loss: 0.8291776776313782
Step 30 | grad_norm: 1.0105177164077759
Step 30 | learning_rate: 9.69409282700422e-05
Step 30 | epoch: 0.0949367088607595
Step 31 | loss: 0.9867770671844482
Step 31 | grad_norm: 1.1340428590774536
Step 31 | learning_rate: 9.683544303797469e-05
Step 31 | epoch: 0.0981012658227848
Step 32 | loss: 0.8142680525779724
Step 32 | grad_norm: 0.9786964058876038
Step 32 | learning_rate: 9.672995780590718e-05
Step 32 | epoch: 0.10126582278481013
Step 33 | loss: 1.0110082626342773
Step 33 | grad_norm: 1.1880918741226196
Step 33 | learning_rate: 9.662447257383967e-05
Step 33 | epoch: 0.10443037974683544
Step 34 | loss: 0.8371492624282837
Step 34 | grad_norm: 1.786217451095581
Step 34 | learning_rate: 9.651898734177216e-05
Step 34 | epoch: 0.10759493670886076
Step 35 | loss: 1.014087438583374
Step 35 | grad_norm: 1.1098191738128662
Step 35 | learning_rate: 9.641350210970464e-05
Step 35 | epoch: 0.11075949367088607
Step 36 | loss: 0.868859052658081
Step 36 | grad_norm: 1.228212594985962
Step 36 | learning_rate: 9.630801687763713e-05
Step 36 | epoch: 0.11392405063291139
Step 37 | loss: 0.9111238718032837
Step 37 | grad_norm: 0.9187421202659607
Step 37 | learning_rate: 9.620253164556962e-05
Step 37 | epoch: 0.11708860759493671
Step 38 | loss: 0.7519440650939941
Step 38 | grad_norm: 0.9265972971916199
Step 38 | learning_rate: 9.609704641350211e-05
Step 38 | epoch: 0.12025316455696203
Step 39 | loss: 0.9024563431739807
Step 39 | grad_norm: 1.0981110334396362
Step 39 | learning_rate: 9.599156118143461e-05
Step 39 | epoch: 0.12341772151898735
Step 40 | loss: 0.885199785232544
Step 40 | grad_norm: 0.8966366052627563
Step 40 | learning_rate: 9.58860759493671e-05
Step 40 | epoch: 0.12658227848101267
Step 41 | loss: 1.119422435760498
Step 41 | grad_norm: 0.8729840517044067
Step 41 | learning_rate: 9.578059071729957e-05
Step 41 | epoch: 0.12974683544303797
Step 42 | loss: 0.889989972114563
Step 42 | grad_norm: 0.9909836649894714
Step 42 | learning_rate: 9.567510548523208e-05
Step 42 | epoch: 0.13291139240506328
Step 43 | loss: 0.9704503417015076
Step 43 | grad_norm: 1.1030418872833252
Step 43 | learning_rate: 9.556962025316456e-05
Step 43 | epoch: 0.1360759493670886
Step 44 | loss: 0.8587315082550049
Step 44 | grad_norm: 1.0343924760818481
Step 44 | learning_rate: 9.546413502109705e-05
Step 44 | epoch: 0.13924050632911392
Step 45 | loss: 0.9545836448669434
Step 45 | grad_norm: 0.7652842998504639
Step 45 | learning_rate: 9.535864978902954e-05
Step 45 | epoch: 0.14240506329113925
Step 46 | loss: 0.9222115278244019
Step 46 | grad_norm: 0.9524170756340027
Step 46 | learning_rate: 9.525316455696203e-05
Step 46 | epoch: 0.14556962025316456
Step 47 | loss: 0.7466107606887817
Step 47 | grad_norm: 0.848092257976532
Step 47 | learning_rate: 9.514767932489452e-05
Step 47 | epoch: 0.14873417721518986
Step 48 | loss: 0.7422422170639038
Step 48 | grad_norm: 0.7142490148544312
Step 48 | learning_rate: 9.5042194092827e-05
Step 48 | epoch: 0.1518987341772152
Step 49 | loss: 0.9326860308647156
Step 49 | grad_norm: 1.0656721591949463
Step 49 | learning_rate: 9.493670886075949e-05
Step 49 | epoch: 0.1550632911392405
Step 50 | loss: 0.8602771162986755
Step 50 | grad_norm: 0.8746232986450195
Step 50 | learning_rate: 9.4831223628692e-05
Step 50 | epoch: 0.15822784810126583
Step 51 | loss: 0.80167555809021
Step 51 | grad_norm: 0.7306644916534424
Step 51 | learning_rate: 9.472573839662447e-05
Step 51 | epoch: 0.16139240506329114
Step 52 | loss: 1.1585252285003662
Step 52 | grad_norm: 0.9962577819824219
Step 52 | learning_rate: 9.462025316455697e-05
Step 52 | epoch: 0.16455696202531644
Step 53 | loss: 0.6808487772941589
Step 53 | grad_norm: 0.7454513311386108
Step 53 | learning_rate: 9.451476793248946e-05
Step 53 | epoch: 0.16772151898734178
Step 54 | loss: 0.8524813055992126
Step 54 | grad_norm: 0.944832444190979
Step 54 | learning_rate: 9.440928270042195e-05
Step 54 | epoch: 0.17088607594936708
Step 55 | loss: 0.9011080861091614
Step 55 | grad_norm: 0.9966081976890564
Step 55 | learning_rate: 9.430379746835444e-05
Step 55 | epoch: 0.17405063291139242
Step 56 | loss: 0.9117679595947266
Step 56 | grad_norm: 1.0079604387283325
Step 56 | learning_rate: 9.419831223628692e-05
Step 56 | epoch: 0.17721518987341772
Step 57 | loss: 0.9340195655822754
Step 57 | grad_norm: 1.1463640928268433
Step 57 | learning_rate: 9.409282700421943e-05
Step 57 | epoch: 0.18037974683544303
Step 58 | loss: 0.8664499521255493
Step 58 | grad_norm: 1.0196267366409302
Step 58 | learning_rate: 9.39873417721519e-05
Step 58 | epoch: 0.18354430379746836
Step 59 | loss: 0.8848355412483215
Step 59 | grad_norm: 1.2167913913726807
Step 59 | learning_rate: 9.388185654008439e-05
Step 59 | epoch: 0.18670886075949367
Step 60 | loss: 0.8477383255958557
Step 60 | grad_norm: 0.8594864010810852
Step 60 | learning_rate: 9.377637130801689e-05
Step 60 | epoch: 0.189873417721519
Step 61 | loss: 0.7235490679740906
Step 61 | grad_norm: 0.7700992822647095
Step 61 | learning_rate: 9.367088607594936e-05
Step 61 | epoch: 0.1930379746835443
Step 62 | loss: 0.7898365259170532
Step 62 | grad_norm: 0.9216981530189514
Step 62 | learning_rate: 9.356540084388185e-05
Step 62 | epoch: 0.1962025316455696
Step 63 | loss: 0.7031621336936951
Step 63 | grad_norm: 1.166954755783081
Step 63 | learning_rate: 9.345991561181435e-05
Step 63 | epoch: 0.19936708860759494
Step 64 | loss: 0.8783487677574158
Step 64 | grad_norm: 1.086424469947815
Step 64 | learning_rate: 9.335443037974684e-05
Step 64 | epoch: 0.20253164556962025
Step 65 | loss: 0.7727378606796265
Step 65 | grad_norm: 0.8745635747909546
Step 65 | learning_rate: 9.324894514767933e-05
Step 65 | epoch: 0.20569620253164558
Step 66 | loss: 1.1507512331008911
Step 66 | grad_norm: 1.009813904762268
Step 66 | learning_rate: 9.314345991561182e-05
Step 66 | epoch: 0.2088607594936709
Step 67 | loss: 1.2970216274261475
Step 67 | grad_norm: 1.2066709995269775
Step 67 | learning_rate: 9.303797468354431e-05
Step 67 | epoch: 0.2120253164556962
Step 68 | loss: 0.8584421873092651
Step 68 | grad_norm: 0.9070467352867126
Step 68 | learning_rate: 9.29324894514768e-05
Step 68 | epoch: 0.21518987341772153
Step 69 | loss: 0.8345344066619873
Step 69 | grad_norm: 0.9247689843177795
Step 69 | learning_rate: 9.282700421940928e-05
Step 69 | epoch: 0.21835443037974683
Step 70 | loss: 0.7773630023002625
Step 70 | grad_norm: 1.3271502256393433
Step 70 | learning_rate: 9.272151898734177e-05
Step 70 | epoch: 0.22151898734177214
Step 71 | loss: 0.824965238571167
Step 71 | grad_norm: 0.89457106590271
Step 71 | learning_rate: 9.261603375527427e-05
Step 71 | epoch: 0.22468354430379747
Step 72 | loss: 0.8098645806312561
Step 72 | grad_norm: 0.8874509334564209
Step 72 | learning_rate: 9.251054852320675e-05
Step 72 | epoch: 0.22784810126582278
Step 73 | loss: 0.9736897945404053
Step 73 | grad_norm: 0.9750681519508362
Step 73 | learning_rate: 9.240506329113925e-05
Step 73 | epoch: 0.2310126582278481
Step 74 | loss: 0.7168033123016357
Step 74 | grad_norm: 1.0009795427322388
Step 74 | learning_rate: 9.229957805907174e-05
Step 74 | epoch: 0.23417721518987342
Step 75 | loss: 0.761420726776123
Step 75 | grad_norm: 1.0001007318496704
Step 75 | learning_rate: 9.219409282700421e-05
Step 75 | epoch: 0.23734177215189872
Step 76 | loss: 0.6974167227745056
Step 76 | grad_norm: 1.1840205192565918
Step 76 | learning_rate: 9.208860759493671e-05
Step 76 | epoch: 0.24050632911392406
Step 77 | loss: 0.6459599137306213
Step 77 | grad_norm: 0.7411468625068665
Step 77 | learning_rate: 9.19831223628692e-05
Step 77 | epoch: 0.24367088607594936
Step 78 | loss: 0.7422580718994141
Step 78 | grad_norm: 1.0017024278640747
Step 78 | learning_rate: 9.187763713080169e-05
Step 78 | epoch: 0.2468354430379747
Step 79 | loss: 0.8408176898956299
Step 79 | grad_norm: 0.9095967411994934
Step 79 | learning_rate: 9.177215189873418e-05
Step 79 | epoch: 0.25
Step 80 | loss: 0.8668079972267151
Step 80 | grad_norm: 0.9954429864883423
Step 80 | learning_rate: 9.166666666666667e-05
Step 80 | epoch: 0.25316455696202533
Step 81 | loss: 0.7993907332420349
Step 81 | grad_norm: 0.9143238067626953
Step 81 | learning_rate: 9.156118143459917e-05
Step 81 | epoch: 0.2563291139240506
Step 82 | loss: 0.8962537050247192
Step 82 | grad_norm: 1.1584296226501465
Step 82 | learning_rate: 9.145569620253164e-05
Step 82 | epoch: 0.25949367088607594
Step 83 | loss: 0.9231811761856079
Step 83 | grad_norm: 1.0749834775924683
Step 83 | learning_rate: 9.135021097046413e-05
Step 83 | epoch: 0.2626582278481013
Step 84 | loss: 0.8540974855422974
Step 84 | grad_norm: 1.0422669649124146
Step 84 | learning_rate: 9.124472573839663e-05
Step 84 | epoch: 0.26582278481012656
Step 85 | loss: 1.0140599012374878
Step 85 | grad_norm: 1.775606393814087
Step 85 | learning_rate: 9.113924050632912e-05
Step 85 | epoch: 0.2689873417721519
Step 86 | loss: 0.8087007999420166
Step 86 | grad_norm: 1.0162721872329712
Step 86 | learning_rate: 9.103375527426161e-05
Step 86 | epoch: 0.2721518987341772
Step 87 | loss: 0.7413169145584106
Step 87 | grad_norm: 1.061191439628601
Step 87 | learning_rate: 9.09282700421941e-05
Step 87 | epoch: 0.27531645569620256
Step 88 | loss: 0.8211747407913208
Step 88 | grad_norm: 1.0323610305786133
Step 88 | learning_rate: 9.082278481012659e-05
Step 88 | epoch: 0.27848101265822783
Step 89 | loss: 0.8132992386817932
Step 89 | grad_norm: 0.9196889400482178
Step 89 | learning_rate: 9.071729957805908e-05
Step 89 | epoch: 0.28164556962025317
Step 90 | loss: 0.9448592662811279
Step 90 | grad_norm: 0.9950587153434753
Step 90 | learning_rate: 9.061181434599156e-05
Step 90 | epoch: 0.2848101265822785
Step 91 | loss: 0.7604300379753113
Step 91 | grad_norm: 0.8434456586837769
Step 91 | learning_rate: 9.050632911392407e-05
Step 91 | epoch: 0.2879746835443038
Step 92 | loss: 0.9630275368690491
Step 92 | grad_norm: 0.9144343733787537
Step 92 | learning_rate: 9.040084388185654e-05
Step 92 | epoch: 0.2911392405063291
Step 93 | loss: 0.8761672973632812
Step 93 | grad_norm: 0.882656455039978
Step 93 | learning_rate: 9.029535864978903e-05
Step 93 | epoch: 0.29430379746835444
Step 94 | loss: 0.7570205926895142
Step 94 | grad_norm: 0.7972260117530823
Step 94 | learning_rate: 9.018987341772153e-05
Step 94 | epoch: 0.2974683544303797
Step 95 | loss: 0.8243594169616699
Step 95 | grad_norm: 0.9266433715820312
Step 95 | learning_rate: 9.008438818565402e-05
Step 95 | epoch: 0.30063291139240506
Step 96 | loss: 0.9318135976791382
Step 96 | grad_norm: 1.0596216917037964
Step 96 | learning_rate: 8.997890295358649e-05
Step 96 | epoch: 0.3037974683544304
Step 97 | loss: 0.8822939991950989
Step 97 | grad_norm: 1.0428918600082397
Step 97 | learning_rate: 8.9873417721519e-05
Step 97 | epoch: 0.3069620253164557
Step 98 | loss: 0.7057663202285767
Step 98 | grad_norm: 0.7723357081413269
Step 98 | learning_rate: 8.976793248945148e-05
Step 98 | epoch: 0.310126582278481
Step 99 | loss: 0.8025825619697571
Step 99 | grad_norm: 0.8960248231887817
Step 99 | learning_rate: 8.966244725738397e-05
Step 99 | epoch: 0.31329113924050633
Step 100 | loss: 0.913173496723175
Step 100 | grad_norm: 1.2609179019927979
Step 100 | learning_rate: 8.955696202531646e-05
Step 100 | epoch: 0.31645569620253167
Step 101 | loss: 0.9945685267448425
Step 101 | grad_norm: 1.0880675315856934
Step 101 | learning_rate: 8.945147679324895e-05
Step 101 | epoch: 0.31962025316455694
Step 102 | loss: 0.8912330865859985
Step 102 | grad_norm: 1.0456874370574951
Step 102 | learning_rate: 8.934599156118144e-05
Step 102 | epoch: 0.3227848101265823
Step 103 | loss: 0.7932621240615845
Step 103 | grad_norm: 1.1653105020523071
Step 103 | learning_rate: 8.924050632911392e-05
Step 103 | epoch: 0.3259493670886076
Step 104 | loss: 0.7217702269554138
Step 104 | grad_norm: 0.8227722644805908
Step 104 | learning_rate: 8.913502109704643e-05
Step 104 | epoch: 0.3291139240506329
Step 105 | loss: 0.7647221684455872
Step 105 | grad_norm: 0.998958945274353
Step 105 | learning_rate: 8.902953586497891e-05
Step 105 | epoch: 0.3322784810126582
Step 106 | loss: 0.7986447811126709
Step 106 | grad_norm: 0.9384914636611938
Step 106 | learning_rate: 8.892405063291139e-05
Step 106 | epoch: 0.33544303797468356
Step 107 | loss: 0.9827253222465515
Step 107 | grad_norm: 1.2851999998092651
Step 107 | learning_rate: 8.881856540084389e-05
Step 107 | epoch: 0.33860759493670883
Step 108 | loss: 0.8282502889633179
Step 108 | grad_norm: 1.3593928813934326
Step 108 | learning_rate: 8.871308016877638e-05
Step 108 | epoch: 0.34177215189873417
Step 109 | loss: 0.7744549512863159
Step 109 | grad_norm: 0.8306300640106201
Step 109 | learning_rate: 8.860759493670887e-05
Step 109 | epoch: 0.3449367088607595
Step 110 | loss: 0.8902781009674072
Step 110 | grad_norm: 0.9322177171707153
Step 110 | learning_rate: 8.850210970464135e-05
Step 110 | epoch: 0.34810126582278483
Step 111 | loss: 0.8686093091964722
Step 111 | grad_norm: 1.2449977397918701
Step 111 | learning_rate: 8.839662447257384e-05
Step 111 | epoch: 0.3512658227848101
Step 112 | loss: 1.0101433992385864
Step 112 | grad_norm: 1.2564352750778198
Step 112 | learning_rate: 8.829113924050633e-05
Step 112 | epoch: 0.35443037974683544
Step 113 | loss: 0.7009725570678711
Step 113 | grad_norm: 0.7482603788375854
Step 113 | learning_rate: 8.818565400843882e-05
Step 113 | epoch: 0.3575949367088608
Step 114 | loss: 0.8714832067489624
Step 114 | grad_norm: 1.1156121492385864
Step 114 | learning_rate: 8.808016877637131e-05
Step 114 | epoch: 0.36075949367088606
Step 115 | loss: 0.8833199739456177
Step 115 | grad_norm: 1.4591615200042725
Step 115 | learning_rate: 8.797468354430381e-05
Step 115 | epoch: 0.3639240506329114
Step 116 | loss: 0.7690529227256775
Step 116 | grad_norm: 0.7233820557594299
Step 116 | learning_rate: 8.786919831223628e-05
Step 116 | epoch: 0.3670886075949367
Step 117 | loss: 0.9059977531433105
Step 117 | grad_norm: 1.3600122928619385
Step 117 | learning_rate: 8.776371308016879e-05
Step 117 | epoch: 0.370253164556962
Step 118 | loss: 0.8283734917640686
Step 118 | grad_norm: 1.251725673675537
Step 118 | learning_rate: 8.765822784810127e-05
Step 118 | epoch: 0.37341772151898733
Step 119 | loss: 0.8904234170913696
Step 119 | grad_norm: 1.1040583848953247
Step 119 | learning_rate: 8.755274261603376e-05
Step 119 | epoch: 0.37658227848101267
Step 120 | loss: 0.7018359303474426
Step 120 | grad_norm: 0.9978998899459839
Step 120 | learning_rate: 8.744725738396625e-05
Step 120 | epoch: 0.379746835443038
Step 121 | loss: 0.7716526985168457
Step 121 | grad_norm: 1.1587072610855103
Step 121 | learning_rate: 8.734177215189874e-05
Step 121 | epoch: 0.3829113924050633
Step 122 | loss: 0.8124445080757141
Step 122 | grad_norm: 1.1588060855865479
Step 122 | learning_rate: 8.723628691983123e-05
Step 122 | epoch: 0.3860759493670886
Step 123 | loss: 0.6980329751968384
Step 123 | grad_norm: 0.9744156002998352
Step 123 | learning_rate: 8.713080168776371e-05
Step 123 | epoch: 0.38924050632911394
Step 124 | loss: 0.7119061946868896
Step 124 | grad_norm: 1.1017155647277832
Step 124 | learning_rate: 8.70253164556962e-05
Step 124 | epoch: 0.3924050632911392
Step 125 | loss: 0.6890230178833008
Step 125 | grad_norm: 0.87476646900177
Step 125 | learning_rate: 8.69198312236287e-05
Step 125 | epoch: 0.39556962025316456
Step 126 | loss: 0.8034787178039551
Step 126 | grad_norm: 1.0145126581192017
Step 126 | learning_rate: 8.681434599156118e-05
Step 126 | epoch: 0.3987341772151899
Step 127 | loss: 0.7580253481864929
Step 127 | grad_norm: 0.9590311646461487
Step 127 | learning_rate: 8.670886075949367e-05
Step 127 | epoch: 0.40189873417721517
Step 128 | loss: 0.8030622005462646
Step 128 | grad_norm: 0.9673364162445068
Step 128 | learning_rate: 8.660337552742617e-05
Step 128 | epoch: 0.4050632911392405
Step 129 | loss: 0.8586025834083557
Step 129 | grad_norm: 1.0593430995941162
Step 129 | learning_rate: 8.649789029535866e-05
Step 129 | epoch: 0.40822784810126583
Step 130 | loss: 0.8998503684997559
Step 130 | grad_norm: 1.2850409746170044
Step 130 | learning_rate: 8.639240506329115e-05
Step 130 | epoch: 0.41139240506329117
Step 131 | loss: 0.7736178040504456
Step 131 | grad_norm: 1.168354868888855
Step 131 | learning_rate: 8.628691983122363e-05
Step 131 | epoch: 0.41455696202531644
Step 132 | loss: 0.717327892780304
Step 132 | grad_norm: 1.6598937511444092
Step 132 | learning_rate: 8.618143459915612e-05
Step 132 | epoch: 0.4177215189873418
Step 133 | loss: 0.8017873764038086
Step 133 | grad_norm: 0.9195502400398254
Step 133 | learning_rate: 8.607594936708861e-05
Step 133 | epoch: 0.4208860759493671
Step 134 | loss: 0.8240911364555359
Step 134 | grad_norm: 0.9016736149787903
Step 134 | learning_rate: 8.59704641350211e-05
Step 134 | epoch: 0.4240506329113924
Step 135 | loss: 0.8157906532287598
Step 135 | grad_norm: 1.01966392993927
Step 135 | learning_rate: 8.586497890295359e-05
Step 135 | epoch: 0.4272151898734177
Step 136 | loss: 0.8268077373504639
Step 136 | grad_norm: 1.1390740871429443
Step 136 | learning_rate: 8.575949367088609e-05
Step 136 | epoch: 0.43037974683544306
Step 137 | loss: 0.8330687284469604
Step 137 | grad_norm: 0.9598175883293152
Step 137 | learning_rate: 8.565400843881856e-05
Step 137 | epoch: 0.43354430379746833
Step 138 | loss: 0.7328115105628967
Step 138 | grad_norm: 0.9122381210327148
Step 138 | learning_rate: 8.554852320675106e-05
Step 138 | epoch: 0.43670886075949367
Step 139 | loss: 0.6291142702102661
Step 139 | grad_norm: 0.9373294711112976
Step 139 | learning_rate: 8.544303797468355e-05
Step 139 | epoch: 0.439873417721519
Step 140 | loss: 0.7817654609680176
Step 140 | grad_norm: 0.846849262714386
Step 140 | learning_rate: 8.533755274261603e-05
Step 140 | epoch: 0.4430379746835443
Step 141 | loss: 0.835002601146698
Step 141 | grad_norm: 1.0055946111679077
Step 141 | learning_rate: 8.523206751054853e-05
Step 141 | epoch: 0.4462025316455696
Step 142 | loss: 0.7189812660217285
Step 142 | grad_norm: 0.8867826461791992
Step 142 | learning_rate: 8.512658227848102e-05
Step 142 | epoch: 0.44936708860759494
Step 143 | loss: 0.7334820032119751
Step 143 | grad_norm: 0.9921507239341736
Step 143 | learning_rate: 8.50210970464135e-05
Step 143 | epoch: 0.4525316455696203
Step 144 | loss: 0.6766867637634277
Step 144 | grad_norm: 0.7664365172386169
Step 144 | learning_rate: 8.4915611814346e-05
Step 144 | epoch: 0.45569620253164556
Step 145 | loss: 0.741208553314209
Step 145 | grad_norm: 0.8108432292938232
Step 145 | learning_rate: 8.481012658227848e-05
Step 145 | epoch: 0.4588607594936709
Step 146 | loss: 0.5676254034042358
Step 146 | grad_norm: 0.8533305525779724
Step 146 | learning_rate: 8.470464135021098e-05
Step 146 | epoch: 0.4620253164556962
Step 147 | loss: 0.8104081153869629
Step 147 | grad_norm: 0.9003473520278931
Step 147 | learning_rate: 8.459915611814346e-05
Step 147 | epoch: 0.4651898734177215
Step 148 | loss: 0.691259503364563
Step 148 | grad_norm: 0.9475741386413574
Step 148 | learning_rate: 8.449367088607595e-05
Step 148 | epoch: 0.46835443037974683
Step 149 | loss: 0.803383469581604
Step 149 | grad_norm: 0.9754049181938171
Step 149 | learning_rate: 8.438818565400845e-05
Step 149 | epoch: 0.47151898734177217
Step 150 | loss: 0.7761229276657104
Step 150 | grad_norm: 0.8584349155426025
Step 150 | learning_rate: 8.428270042194094e-05
Step 150 | epoch: 0.47468354430379744
Step 151 | loss: 0.8078223466873169
Step 151 | grad_norm: 1.15945565700531
Step 151 | learning_rate: 8.417721518987342e-05
Step 151 | epoch: 0.4778481012658228
Step 152 | loss: 0.9220563769340515
Step 152 | grad_norm: 1.0909448862075806
Step 152 | learning_rate: 8.407172995780591e-05
Step 152 | epoch: 0.4810126582278481
Step 153 | loss: 0.855228066444397
Step 153 | grad_norm: 1.0574448108673096
Step 153 | learning_rate: 8.39662447257384e-05
Step 153 | epoch: 0.48417721518987344
Step 154 | loss: 0.8545065522193909
Step 154 | grad_norm: 0.944476842880249
Step 154 | learning_rate: 8.386075949367089e-05
Step 154 | epoch: 0.4873417721518987
Step 155 | loss: 0.830756425857544
Step 155 | grad_norm: 1.3854378461837769
Step 155 | learning_rate: 8.375527426160338e-05
Step 155 | epoch: 0.49050632911392406
Step 156 | loss: 0.9367811679840088
Step 156 | grad_norm: 1.097609043121338
Step 156 | learning_rate: 8.364978902953588e-05
Step 156 | epoch: 0.4936708860759494
Step 157 | loss: 0.892875075340271
Step 157 | grad_norm: 1.0920614004135132
Step 157 | learning_rate: 8.354430379746835e-05
Step 157 | epoch: 0.49683544303797467
Step 158 | loss: 0.6645914912223816
Step 158 | grad_norm: 0.781548023223877
Step 158 | learning_rate: 8.343881856540084e-05
Step 158 | epoch: 0.5
Step 159 | loss: 0.8119387030601501
Step 159 | grad_norm: 0.9490334391593933
Step 159 | learning_rate: 8.333333333333334e-05
Step 159 | epoch: 0.5031645569620253
Step 160 | loss: 0.7839598059654236
Step 160 | grad_norm: 1.1664353609085083
Step 160 | learning_rate: 8.322784810126583e-05
Step 160 | epoch: 0.5063291139240507
Step 161 | loss: 0.7791200280189514
Step 161 | grad_norm: 0.791305422782898
Step 161 | learning_rate: 8.312236286919831e-05
Step 161 | epoch: 0.509493670886076
Step 162 | loss: 0.7866835594177246
Step 162 | grad_norm: 1.0741932392120361
Step 162 | learning_rate: 8.301687763713081e-05
Step 162 | epoch: 0.5126582278481012
Step 163 | loss: 0.8230146765708923
Step 163 | grad_norm: 0.963222861289978
Step 163 | learning_rate: 8.29113924050633e-05
Step 163 | epoch: 0.5158227848101266
Step 164 | loss: 0.6980335712432861
Step 164 | grad_norm: 0.8214691877365112
Step 164 | learning_rate: 8.280590717299579e-05
Step 164 | epoch: 0.5189873417721519
Step 165 | loss: 0.8795064687728882
Step 165 | grad_norm: 0.9225180149078369
Step 165 | learning_rate: 8.270042194092827e-05
Step 165 | epoch: 0.5221518987341772
Step 166 | loss: 0.6789273619651794
Step 166 | grad_norm: 0.8266125917434692
Step 166 | learning_rate: 8.259493670886076e-05
Step 166 | epoch: 0.5253164556962026
Step 167 | loss: 0.7872726917266846
Step 167 | grad_norm: 0.8770735263824463
Step 167 | learning_rate: 8.248945147679325e-05
Step 167 | epoch: 0.5284810126582279
Step 168 | loss: 0.7080971002578735
Step 168 | grad_norm: 0.8424050807952881
Step 168 | learning_rate: 8.238396624472574e-05
Step 168 | epoch: 0.5316455696202531
Step 169 | loss: 0.6052396297454834
Step 169 | grad_norm: 0.7682484984397888
Step 169 | learning_rate: 8.227848101265824e-05
Step 169 | epoch: 0.5348101265822784
Step 170 | loss: 0.8590015769004822
Step 170 | grad_norm: 1.1214137077331543
Step 170 | learning_rate: 8.217299578059073e-05
Step 170 | epoch: 0.5379746835443038
Step 171 | loss: 0.7245519161224365
Step 171 | grad_norm: 0.8663696646690369
Step 171 | learning_rate: 8.20675105485232e-05
Step 171 | epoch: 0.5411392405063291
Step 172 | loss: 0.6610605120658875
Step 172 | grad_norm: 0.8401000499725342
Step 172 | learning_rate: 8.19620253164557e-05
Step 172 | epoch: 0.5443037974683544
Step 173 | loss: 0.7742705345153809
Step 173 | grad_norm: 0.9571190476417542
Step 173 | learning_rate: 8.185654008438819e-05
Step 173 | epoch: 0.5474683544303798
Step 174 | loss: 0.6408815383911133
Step 174 | grad_norm: 0.805942177772522
Step 174 | learning_rate: 8.175105485232068e-05
Step 174 | epoch: 0.5506329113924051
Step 175 | loss: 0.7733176946640015
Step 175 | grad_norm: 1.14802885055542
Step 175 | learning_rate: 8.164556962025317e-05
Step 175 | epoch: 0.5537974683544303
Step 176 | loss: 0.7903620004653931
Step 176 | grad_norm: 0.9764444231987
Step 176 | learning_rate: 8.154008438818566e-05
Step 176 | epoch: 0.5569620253164557
Step 177 | loss: 0.6897692084312439
Step 177 | grad_norm: 0.8609745502471924
Step 177 | learning_rate: 8.143459915611815e-05
Step 177 | epoch: 0.560126582278481
Step 178 | loss: 0.7242370843887329
Step 178 | grad_norm: 1.2473716735839844
Step 178 | learning_rate: 8.132911392405063e-05
Step 178 | epoch: 0.5632911392405063
Step 179 | loss: 0.7440435886383057
Step 179 | grad_norm: 0.9461742639541626
Step 179 | learning_rate: 8.122362869198312e-05
Step 179 | epoch: 0.5664556962025317
Step 180 | loss: 0.7974015474319458
Step 180 | grad_norm: 1.0764446258544922
Step 180 | learning_rate: 8.111814345991562e-05
Step 180 | epoch: 0.569620253164557
Step 181 | loss: 0.6599466800689697
Step 181 | grad_norm: 0.7574780583381653
Step 181 | learning_rate: 8.10126582278481e-05
Step 181 | epoch: 0.5727848101265823
Step 182 | loss: 0.7506310343742371
Step 182 | grad_norm: 0.9540454745292664
Step 182 | learning_rate: 8.090717299578059e-05
Step 182 | epoch: 0.5759493670886076
Step 183 | loss: 0.8358263969421387
Step 183 | grad_norm: 1.5581591129302979
Step 183 | learning_rate: 8.080168776371309e-05
Step 183 | epoch: 0.5791139240506329
Step 184 | loss: 0.6909445524215698
Step 184 | grad_norm: 1.0164573192596436
Step 184 | learning_rate: 8.069620253164558e-05
Step 184 | epoch: 0.5822784810126582
Step 185 | loss: 0.9622402191162109
Step 185 | grad_norm: 1.5929316282272339
Step 185 | learning_rate: 8.059071729957806e-05
Step 185 | epoch: 0.5854430379746836
Step 186 | loss: 0.9443315267562866
Step 186 | grad_norm: 1.0275449752807617
Step 186 | learning_rate: 8.048523206751055e-05
Step 186 | epoch: 0.5886075949367089
Step 187 | loss: 0.7444382905960083
Step 187 | grad_norm: 0.9066394567489624
Step 187 | learning_rate: 8.037974683544304e-05
Step 187 | epoch: 0.5917721518987342
Step 188 | loss: 0.7410168647766113
Step 188 | grad_norm: 0.8151522278785706
Step 188 | learning_rate: 8.027426160337553e-05
Step 188 | epoch: 0.5949367088607594
Step 189 | loss: 0.8681631684303284
Step 189 | grad_norm: 1.2192237377166748
Step 189 | learning_rate: 8.016877637130802e-05
Step 189 | epoch: 0.5981012658227848
Step 190 | loss: 0.6172375679016113
Step 190 | grad_norm: 0.7097450494766235
Step 190 | learning_rate: 8.006329113924052e-05
Step 190 | epoch: 0.6012658227848101
Step 191 | loss: 0.7250989079475403
Step 191 | grad_norm: 0.9136959910392761
Step 191 | learning_rate: 7.9957805907173e-05
Step 191 | epoch: 0.6044303797468354
Step 192 | loss: 0.7037632465362549
Step 192 | grad_norm: 0.9122689366340637
Step 192 | learning_rate: 7.985232067510548e-05
Step 192 | epoch: 0.6075949367088608
Step 193 | loss: 0.9317439794540405
Step 193 | grad_norm: 1.2929303646087646
Step 193 | learning_rate: 7.974683544303798e-05
Step 193 | epoch: 0.6107594936708861
Step 194 | loss: 0.7467870712280273
Step 194 | grad_norm: 0.9207684397697449
Step 194 | learning_rate: 7.964135021097047e-05
Step 194 | epoch: 0.6139240506329114
Step 195 | loss: 0.8206231594085693
Step 195 | grad_norm: 0.8178504109382629
Step 195 | learning_rate: 7.953586497890295e-05
Step 195 | epoch: 0.6170886075949367
Step 196 | loss: 0.7311117053031921
Step 196 | grad_norm: 0.8183388113975525
Step 196 | learning_rate: 7.943037974683545e-05
Step 196 | epoch: 0.620253164556962
Step 197 | loss: 0.8879643678665161
Step 197 | grad_norm: 1.0250802040100098
Step 197 | learning_rate: 7.932489451476794e-05
Step 197 | epoch: 0.6234177215189873
Step 198 | loss: 0.8059399127960205
Step 198 | grad_norm: 1.1640318632125854
Step 198 | learning_rate: 7.921940928270042e-05
Step 198 | epoch: 0.6265822784810127
Step 199 | loss: 0.7122057676315308
Step 199 | grad_norm: 0.8401491045951843
Step 199 | learning_rate: 7.911392405063291e-05
Step 199 | epoch: 0.629746835443038
Step 200 | loss: 0.8180584907531738
Step 200 | grad_norm: 0.876981258392334
Step 200 | learning_rate: 7.90084388185654e-05
Step 200 | epoch: 0.6329113924050633
Step 201 | loss: 0.8145291805267334
Step 201 | grad_norm: 1.0192580223083496
Step 201 | learning_rate: 7.89029535864979e-05
Step 201 | epoch: 0.6360759493670886
Step 202 | loss: 0.7070013284683228
Step 202 | grad_norm: 0.9114918112754822
Step 202 | learning_rate: 7.879746835443038e-05
Step 202 | epoch: 0.6392405063291139
Step 203 | loss: 0.7883630394935608
Step 203 | grad_norm: 0.9181415438652039
Step 203 | learning_rate: 7.869198312236288e-05
Step 203 | epoch: 0.6424050632911392
Step 204 | loss: 0.8432339429855347
Step 204 | grad_norm: 1.2162306308746338
Step 204 | learning_rate: 7.858649789029537e-05
Step 204 | epoch: 0.6455696202531646
Step 205 | loss: 0.7292966842651367
Step 205 | grad_norm: 0.8282127380371094
Step 205 | learning_rate: 7.848101265822784e-05
Step 205 | epoch: 0.6487341772151899
Step 206 | loss: 0.7099107503890991
Step 206 | grad_norm: 0.8207399249076843
Step 206 | learning_rate: 7.837552742616034e-05
Step 206 | epoch: 0.6518987341772152
Step 207 | loss: 0.740675687789917
Step 207 | grad_norm: 0.8222335577011108
Step 207 | learning_rate: 7.827004219409283e-05
Step 207 | epoch: 0.6550632911392406
Step 208 | loss: 0.5648699998855591
Step 208 | grad_norm: 0.8255237340927124
Step 208 | learning_rate: 7.816455696202532e-05
Step 208 | epoch: 0.6582278481012658
Step 209 | loss: 0.7615331411361694
Step 209 | grad_norm: 0.8742075562477112
Step 209 | learning_rate: 7.805907172995781e-05
Step 209 | epoch: 0.6613924050632911
Step 210 | loss: 0.831996738910675
Step 210 | grad_norm: 0.8718453645706177
Step 210 | learning_rate: 7.79535864978903e-05
Step 210 | epoch: 0.6645569620253164
Step 211 | loss: 0.668874204158783
Step 211 | grad_norm: 0.8783906102180481
Step 211 | learning_rate: 7.78481012658228e-05
Step 211 | epoch: 0.6677215189873418
Step 212 | loss: 0.769411563873291
Step 212 | grad_norm: 0.9402977228164673
Step 212 | learning_rate: 7.774261603375527e-05
Step 212 | epoch: 0.6708860759493671
Step 213 | loss: 0.7602274417877197
Step 213 | grad_norm: 1.0744853019714355
Step 213 | learning_rate: 7.763713080168776e-05
Step 213 | epoch: 0.6740506329113924
Step 214 | loss: 0.8128917813301086
Step 214 | grad_norm: 0.9089781641960144
Step 214 | learning_rate: 7.753164556962026e-05
Step 214 | epoch: 0.6772151898734177
Step 215 | loss: 0.6753758192062378
Step 215 | grad_norm: 0.8890783786773682
Step 215 | learning_rate: 7.742616033755275e-05
Step 215 | epoch: 0.680379746835443
Step 216 | loss: 0.71741783618927
Step 216 | grad_norm: 0.8091217875480652
Step 216 | learning_rate: 7.732067510548524e-05
Step 216 | epoch: 0.6835443037974683
Step 217 | loss: 0.9420756101608276
Step 217 | grad_norm: 1.4150631427764893
Step 217 | learning_rate: 7.721518987341773e-05
Step 217 | epoch: 0.6867088607594937
Step 218 | loss: 0.6915392875671387
Step 218 | grad_norm: 0.8066079020500183
Step 218 | learning_rate: 7.710970464135022e-05
Step 218 | epoch: 0.689873417721519
Step 219 | loss: 0.6784504652023315
Step 219 | grad_norm: 0.8161419630050659
Step 219 | learning_rate: 7.70042194092827e-05
Step 219 | epoch: 0.6930379746835443
Step 220 | loss: 0.7784923315048218
Step 220 | grad_norm: 1.118811845779419
Step 220 | learning_rate: 7.689873417721519e-05
Step 220 | epoch: 0.6962025316455697
Step 221 | loss: 0.7958561182022095
Step 221 | grad_norm: 1.1979271173477173
Step 221 | learning_rate: 7.679324894514768e-05
Step 221 | epoch: 0.6993670886075949
Step 222 | loss: 0.6916161775588989
Step 222 | grad_norm: 0.9009000658988953
Step 222 | learning_rate: 7.668776371308017e-05
Step 222 | epoch: 0.7025316455696202
Step 223 | loss: 0.7721455097198486
Step 223 | grad_norm: 0.959001898765564
Step 223 | learning_rate: 7.658227848101266e-05
Step 223 | epoch: 0.7056962025316456
Step 224 | loss: 0.7965304851531982
Step 224 | grad_norm: 0.9834486246109009
Step 224 | learning_rate: 7.647679324894516e-05
Step 224 | epoch: 0.7088607594936709
Step 225 | loss: 0.7343379259109497
Step 225 | grad_norm: 0.9264588952064514
Step 225 | learning_rate: 7.637130801687765e-05
Step 225 | epoch: 0.7120253164556962
Step 226 | loss: 0.8088584542274475
Step 226 | grad_norm: 1.2715755701065063
Step 226 | learning_rate: 7.626582278481012e-05
Step 226 | epoch: 0.7151898734177216
Step 227 | loss: 0.8167132139205933
Step 227 | grad_norm: 1.0161099433898926
Step 227 | learning_rate: 7.616033755274262e-05
Step 227 | epoch: 0.7183544303797469
Step 228 | loss: 0.6456530094146729
Step 228 | grad_norm: 0.752309262752533
Step 228 | learning_rate: 7.605485232067511e-05
Step 228 | epoch: 0.7215189873417721
Step 229 | loss: 0.7865956425666809
Step 229 | grad_norm: 1.2454197406768799
Step 229 | learning_rate: 7.59493670886076e-05
Step 229 | epoch: 0.7246835443037974
Step 230 | loss: 0.6217291355133057
Step 230 | grad_norm: 0.8437262177467346
Step 230 | learning_rate: 7.584388185654009e-05
Step 230 | epoch: 0.7278481012658228
Step 231 | loss: 0.8435367941856384
Step 231 | grad_norm: 1.0117340087890625
Step 231 | learning_rate: 7.573839662447258e-05
Step 231 | epoch: 0.7310126582278481
Step 232 | loss: 0.803321897983551
Step 232 | grad_norm: 1.1611721515655518
Step 232 | learning_rate: 7.563291139240506e-05
Step 232 | epoch: 0.7341772151898734
Step 233 | loss: 0.7227895855903625
Step 233 | grad_norm: 0.841115415096283
Step 233 | learning_rate: 7.552742616033755e-05
Step 233 | epoch: 0.7373417721518988
Step 234 | loss: 0.8827258944511414
Step 234 | grad_norm: 1.0268899202346802
Step 234 | learning_rate: 7.542194092827004e-05
Step 234 | epoch: 0.740506329113924
Step 235 | loss: 0.804534375667572
Step 235 | grad_norm: 0.9821562170982361
Step 235 | learning_rate: 7.531645569620254e-05
Step 235 | epoch: 0.7436708860759493
Step 236 | loss: 0.7611477375030518
Step 236 | grad_norm: 0.9100298285484314
Step 236 | learning_rate: 7.521097046413502e-05
Step 236 | epoch: 0.7468354430379747
Step 237 | loss: 0.7778864502906799
Step 237 | grad_norm: 0.8811132907867432
Step 237 | learning_rate: 7.510548523206752e-05
Step 237 | epoch: 0.75
Step 238 | loss: 0.7697790861129761
Step 238 | grad_norm: 1.299943208694458
Step 238 | learning_rate: 7.500000000000001e-05
Step 238 | epoch: 0.7531645569620253
Step 239 | loss: 0.6959108114242554
Step 239 | grad_norm: 1.3882960081100464
Step 239 | learning_rate: 7.48945147679325e-05
Step 239 | epoch: 0.7563291139240507
Step 240 | loss: 0.7053536772727966
Step 240 | grad_norm: 0.9281969666481018
Step 240 | learning_rate: 7.478902953586498e-05
Step 240 | epoch: 0.759493670886076
Step 241 | loss: 0.804072380065918
Step 241 | grad_norm: 1.273818016052246
Step 241 | learning_rate: 7.468354430379747e-05
Step 241 | epoch: 0.7626582278481012
Step 242 | loss: 0.6417346000671387
Step 242 | grad_norm: 0.7777071595191956
Step 242 | learning_rate: 7.457805907172996e-05
Step 242 | epoch: 0.7658227848101266
Step 243 | loss: 0.7469127774238586
Step 243 | grad_norm: 1.0585970878601074
Step 243 | learning_rate: 7.447257383966245e-05
Step 243 | epoch: 0.7689873417721519
Step 244 | loss: 0.898018479347229
Step 244 | grad_norm: 1.2524467706680298
Step 244 | learning_rate: 7.436708860759494e-05
Step 244 | epoch: 0.7721518987341772
Step 245 | loss: 0.7443637251853943
Step 245 | grad_norm: 0.8737637996673584
Step 245 | learning_rate: 7.426160337552744e-05
Step 245 | epoch: 0.7753164556962026
Step 246 | loss: 0.7489121556282043
Step 246 | grad_norm: 0.8008031249046326
Step 246 | learning_rate: 7.415611814345991e-05
Step 246 | epoch: 0.7784810126582279
Step 247 | loss: 0.5519845485687256
Step 247 | grad_norm: 0.7744277715682983
Step 247 | learning_rate: 7.40506329113924e-05
Step 247 | epoch: 0.7816455696202531
Step 248 | loss: 0.8084707856178284
Step 248 | grad_norm: 1.005014419555664
Step 248 | learning_rate: 7.39451476793249e-05
Step 248 | epoch: 0.7848101265822784
Step 249 | loss: 0.6932245492935181
Step 249 | grad_norm: 0.7481361031532288
Step 249 | learning_rate: 7.383966244725739e-05
Step 249 | epoch: 0.7879746835443038
Step 250 | loss: 0.9185893535614014
Step 250 | grad_norm: 1.0879707336425781
Step 250 | learning_rate: 7.373417721518988e-05
Step 250 | epoch: 0.7911392405063291
Step 251 | loss: 0.6480317115783691
Step 251 | grad_norm: 0.8468449115753174
Step 251 | learning_rate: 7.362869198312237e-05
Step 251 | epoch: 0.7943037974683544
Step 252 | loss: 0.7813605070114136
Step 252 | grad_norm: 0.8414813280105591
Step 252 | learning_rate: 7.352320675105486e-05
Step 252 | epoch: 0.7974683544303798
Step 253 | loss: 0.6655205488204956
Step 253 | grad_norm: 0.8942210078239441
Step 253 | learning_rate: 7.341772151898734e-05
Step 253 | epoch: 0.8006329113924051
Step 254 | loss: 0.7852098941802979
Step 254 | grad_norm: 1.0910677909851074
Step 254 | learning_rate: 7.331223628691983e-05
Step 254 | epoch: 0.8037974683544303
Step 255 | loss: 0.7104268074035645
Step 255 | grad_norm: 1.0034538507461548
Step 255 | learning_rate: 7.320675105485233e-05
Step 255 | epoch: 0.8069620253164557
Step 256 | loss: 0.563091516494751
Step 256 | grad_norm: 0.8241719007492065
Step 256 | learning_rate: 7.310126582278481e-05
Step 256 | epoch: 0.810126582278481
Step 257 | loss: 0.6802831888198853
Step 257 | grad_norm: 1.0508297681808472
Step 257 | learning_rate: 7.29957805907173e-05
Step 257 | epoch: 0.8132911392405063
Step 258 | loss: 0.8686572313308716
Step 258 | grad_norm: 1.0099060535430908
Step 258 | learning_rate: 7.28902953586498e-05
Step 258 | epoch: 0.8164556962025317
Step 259 | loss: 0.8154361248016357
Step 259 | grad_norm: 1.036097764968872
Step 259 | learning_rate: 7.278481012658229e-05
Step 259 | epoch: 0.819620253164557
Step 260 | loss: 0.8520200252532959
Step 260 | grad_norm: 1.1317988634109497
Step 260 | learning_rate: 7.267932489451476e-05
Step 260 | epoch: 0.8227848101265823
Step 261 | loss: 0.6953181028366089
Step 261 | grad_norm: 0.7522098422050476
Step 261 | learning_rate: 7.257383966244726e-05
Step 261 | epoch: 0.8259493670886076
Step 262 | loss: 0.8131512403488159
Step 262 | grad_norm: 0.8537635803222656
Step 262 | learning_rate: 7.246835443037975e-05
Step 262 | epoch: 0.8291139240506329
Step 263 | loss: 0.7925342321395874
Step 263 | grad_norm: 1.0571602582931519
Step 263 | learning_rate: 7.236286919831224e-05
Step 263 | epoch: 0.8322784810126582
Step 264 | loss: 0.6580917835235596
Step 264 | grad_norm: 0.9261075854301453
Step 264 | learning_rate: 7.225738396624473e-05
Step 264 | epoch: 0.8354430379746836
Step 265 | loss: 0.6863174438476562
Step 265 | grad_norm: 0.8020749092102051
Step 265 | learning_rate: 7.215189873417722e-05
Step 265 | epoch: 0.8386075949367089
Step 266 | loss: 0.8547275066375732
Step 266 | grad_norm: 1.0699101686477661
Step 266 | learning_rate: 7.204641350210972e-05
Step 266 | epoch: 0.8417721518987342
Step 267 | loss: 0.7521543502807617
Step 267 | grad_norm: 0.9462093114852905
Step 267 | learning_rate: 7.194092827004219e-05
Step 267 | epoch: 0.8449367088607594
Step 268 | loss: 0.7881501913070679
Step 268 | grad_norm: 0.9442533254623413
Step 268 | learning_rate: 7.18354430379747e-05
Step 268 | epoch: 0.8481012658227848
Step 269 | loss: 0.7314102649688721
Step 269 | grad_norm: 0.9172919988632202
Step 269 | learning_rate: 7.172995780590718e-05
Step 269 | epoch: 0.8512658227848101
Step 270 | loss: 0.7965030670166016
Step 270 | grad_norm: 0.8973910212516785
Step 270 | learning_rate: 7.162447257383966e-05
Step 270 | epoch: 0.8544303797468354
Step 271 | loss: 0.6678435206413269
Step 271 | grad_norm: 0.8160101175308228
Step 271 | learning_rate: 7.151898734177216e-05
Step 271 | epoch: 0.8575949367088608
Step 272 | loss: 0.6521995067596436
Step 272 | grad_norm: 0.8087381720542908
Step 272 | learning_rate: 7.141350210970465e-05
Step 272 | epoch: 0.8607594936708861
Step 273 | loss: 0.9120515584945679
Step 273 | grad_norm: 1.0721853971481323
Step 273 | learning_rate: 7.130801687763713e-05
Step 273 | epoch: 0.8639240506329114
Step 274 | loss: 0.5793229341506958
Step 274 | grad_norm: 0.8723158836364746
Step 274 | learning_rate: 7.120253164556962e-05
Step 274 | epoch: 0.8670886075949367
Step 275 | loss: 0.6711156368255615
Step 275 | grad_norm: 1.028424620628357
Step 275 | learning_rate: 7.109704641350211e-05
Step 275 | epoch: 0.870253164556962
Step 276 | loss: 0.8052818775177002
Step 276 | grad_norm: 1.6617166996002197
Step 276 | learning_rate: 7.099156118143461e-05
Step 276 | epoch: 0.8734177215189873
Step 277 | loss: 0.716098427772522
Step 277 | grad_norm: 0.8979892134666443
Step 277 | learning_rate: 7.088607594936709e-05
Step 277 | epoch: 0.8765822784810127
Step 278 | loss: 0.7382910251617432
Step 278 | grad_norm: 0.8000620603561401
Step 278 | learning_rate: 7.078059071729958e-05
Step 278 | epoch: 0.879746835443038
Step 279 | loss: 0.8174158334732056
Step 279 | grad_norm: 1.0234118700027466
Step 279 | learning_rate: 7.067510548523208e-05
Step 279 | epoch: 0.8829113924050633
Step 280 | loss: 0.756851077079773
Step 280 | grad_norm: 1.0628489255905151
Step 280 | learning_rate: 7.056962025316457e-05
Step 280 | epoch: 0.8860759493670886
Step 281 | loss: 0.6040958166122437
Step 281 | grad_norm: 0.7036790251731873
Step 281 | learning_rate: 7.046413502109705e-05
Step 281 | epoch: 0.8892405063291139
Step 282 | loss: 0.9143377542495728
Step 282 | grad_norm: 0.8920556902885437
Step 282 | learning_rate: 7.035864978902954e-05
Step 282 | epoch: 0.8924050632911392
Step 283 | loss: 0.7606234550476074
Step 283 | grad_norm: 0.785203218460083
Step 283 | learning_rate: 7.025316455696203e-05
Step 283 | epoch: 0.8955696202531646
Step 284 | loss: 0.7843828201293945
Step 284 | grad_norm: 1.4175468683242798
Step 284 | learning_rate: 7.014767932489452e-05
Step 284 | epoch: 0.8987341772151899
Step 285 | loss: 0.5928261280059814
Step 285 | grad_norm: 0.8967345356941223
Step 285 | learning_rate: 7.0042194092827e-05
Step 285 | epoch: 0.9018987341772152
Step 286 | loss: 0.7250180244445801
Step 286 | grad_norm: 1.0469526052474976
Step 286 | learning_rate: 6.99367088607595e-05
Step 286 | epoch: 0.9050632911392406
Step 287 | loss: 0.8149487972259521
Step 287 | grad_norm: 1.0824860334396362
Step 287 | learning_rate: 6.983122362869198e-05
Step 287 | epoch: 0.9082278481012658
Step 288 | loss: 0.6059629917144775
Step 288 | grad_norm: 0.8053820729255676
Step 288 | learning_rate: 6.972573839662447e-05
Step 288 | epoch: 0.9113924050632911
Step 289 | loss: 0.8247916102409363
Step 289 | grad_norm: 0.8725795149803162
Step 289 | learning_rate: 6.962025316455697e-05
Step 289 | epoch: 0.9145569620253164
Step 290 | loss: 0.824270486831665
Step 290 | grad_norm: 0.8850759863853455
Step 290 | learning_rate: 6.951476793248946e-05
Step 290 | epoch: 0.9177215189873418
Step 291 | loss: 0.7781707644462585
Step 291 | grad_norm: 1.054567813873291
Step 291 | learning_rate: 6.940928270042194e-05
Step 291 | epoch: 0.9208860759493671
Step 292 | loss: 0.7906703948974609
Step 292 | grad_norm: 1.002009391784668
Step 292 | learning_rate: 6.930379746835444e-05
Step 292 | epoch: 0.9240506329113924
Step 293 | loss: 0.6277716755867004
Step 293 | grad_norm: 0.9611109495162964
Step 293 | learning_rate: 6.919831223628693e-05
Step 293 | epoch: 0.9272151898734177
Step 294 | loss: 0.818793535232544
Step 294 | grad_norm: 1.1792608499526978
Step 294 | learning_rate: 6.909282700421941e-05
Step 294 | epoch: 0.930379746835443
Step 295 | loss: 0.629524290561676
Step 295 | grad_norm: 1.0102777481079102
Step 295 | learning_rate: 6.89873417721519e-05
Step 295 | epoch: 0.9335443037974683
Step 296 | loss: 0.6628850698471069
Step 296 | grad_norm: 0.8194931149482727
Step 296 | learning_rate: 6.888185654008439e-05
Step 296 | epoch: 0.9367088607594937
Step 297 | loss: 0.6874930262565613
Step 297 | grad_norm: 0.8164809942245483
Step 297 | learning_rate: 6.877637130801688e-05
Step 297 | epoch: 0.939873417721519
Step 298 | loss: 0.7011432647705078
Step 298 | grad_norm: 0.9992108345031738
Step 298 | learning_rate: 6.867088607594937e-05
Step 298 | epoch: 0.9430379746835443
Step 299 | loss: 0.8934488892555237
Step 299 | grad_norm: 1.0897496938705444
Step 299 | learning_rate: 6.856540084388186e-05
Step 299 | epoch: 0.9462025316455697
Step 300 | loss: 0.7907556295394897
Step 300 | grad_norm: 1.0970102548599243
Step 300 | learning_rate: 6.845991561181436e-05
Step 300 | epoch: 0.9493670886075949
Step 301 | loss: 0.5286040902137756
Step 301 | grad_norm: 0.8415144085884094
Step 301 | learning_rate: 6.835443037974683e-05
Step 301 | epoch: 0.9525316455696202
Step 302 | loss: 0.7275087237358093
Step 302 | grad_norm: 0.9425252079963684
Step 302 | learning_rate: 6.824894514767933e-05
Step 302 | epoch: 0.9556962025316456
Step 303 | loss: 0.7274824380874634
Step 303 | grad_norm: 0.8831148743629456
Step 303 | learning_rate: 6.814345991561182e-05
Step 303 | epoch: 0.9588607594936709
Step 304 | loss: 0.7099507451057434
Step 304 | grad_norm: 1.01115882396698
Step 304 | learning_rate: 6.803797468354431e-05
Step 304 | epoch: 0.9620253164556962
Step 305 | loss: 0.7125912308692932
Step 305 | grad_norm: 0.9980749487876892
Step 305 | learning_rate: 6.79324894514768e-05
Step 305 | epoch: 0.9651898734177216
Step 306 | loss: 0.8463693857192993
Step 306 | grad_norm: 1.1400991678237915
Step 306 | learning_rate: 6.782700421940929e-05
Step 306 | epoch: 0.9683544303797469
Step 307 | loss: 0.638276219367981
Step 307 | grad_norm: 0.8260996341705322
Step 307 | learning_rate: 6.772151898734177e-05
Step 307 | epoch: 0.9715189873417721
Step 308 | loss: 0.7598894834518433
Step 308 | grad_norm: 1.136438012123108
Step 308 | learning_rate: 6.761603375527426e-05
Step 308 | epoch: 0.9746835443037974
Step 309 | loss: 0.7384065985679626
Step 309 | grad_norm: 0.9465798139572144
Step 309 | learning_rate: 6.751054852320675e-05
Step 309 | epoch: 0.9778481012658228
Step 310 | loss: 0.7201375961303711
Step 310 | grad_norm: 1.2607879638671875
Step 310 | learning_rate: 6.740506329113925e-05
Step 310 | epoch: 0.9810126582278481
Step 311 | loss: 0.5834346413612366
Step 311 | grad_norm: 0.8429450392723083
Step 311 | learning_rate: 6.729957805907173e-05
Step 311 | epoch: 0.9841772151898734
Step 312 | loss: 0.6698077917098999
Step 312 | grad_norm: 0.8788201808929443
Step 312 | learning_rate: 6.719409282700422e-05
Step 312 | epoch: 0.9873417721518988
Step 313 | loss: 0.6880013942718506
Step 313 | grad_norm: 1.074010968208313
Step 313 | learning_rate: 6.708860759493672e-05
Step 313 | epoch: 0.990506329113924
Step 314 | loss: 0.6515061855316162
Step 314 | grad_norm: 0.8785950541496277
Step 314 | learning_rate: 6.69831223628692e-05
Step 314 | epoch: 0.9936708860759493
Step 315 | loss: 0.783729076385498
Step 315 | grad_norm: 1.1307868957519531
Step 315 | learning_rate: 6.68776371308017e-05
Step 315 | epoch: 0.9968354430379747
Step 316 | loss: 0.6872509717941284
Step 316 | grad_norm: 0.8699610829353333
Step 316 | learning_rate: 6.677215189873418e-05
Step 316 | epoch: 1.0
Step 317 | loss: 0.6952937841415405
Step 317 | grad_norm: 0.9528601169586182
Step 317 | learning_rate: 6.666666666666667e-05
Step 317 | epoch: 1.0031645569620253
Step 318 | loss: 0.5542588829994202
Step 318 | grad_norm: 0.9069606065750122
Step 318 | learning_rate: 6.656118143459916e-05
Step 318 | epoch: 1.0063291139240507
Step 319 | loss: 0.5224575400352478
Step 319 | grad_norm: 0.6696609854698181
Step 319 | learning_rate: 6.645569620253165e-05
Step 319 | epoch: 1.009493670886076
Step 320 | loss: 0.6047447323799133
Step 320 | grad_norm: 1.3284063339233398
Step 320 | learning_rate: 6.635021097046413e-05
Step 320 | epoch: 1.0126582278481013
Step 321 | loss: 0.43086349964141846
Step 321 | grad_norm: 0.737278163433075
Step 321 | learning_rate: 6.624472573839662e-05
Step 321 | epoch: 1.0158227848101267
Step 322 | loss: 0.5578031539916992
Step 322 | grad_norm: 0.7889119982719421
Step 322 | learning_rate: 6.613924050632911e-05
Step 322 | epoch: 1.018987341772152
Step 323 | loss: 0.6311212778091431
Step 323 | grad_norm: 1.380745530128479
Step 323 | learning_rate: 6.603375527426161e-05
Step 323 | epoch: 1.0221518987341771
Step 324 | loss: 0.5639686584472656
Step 324 | grad_norm: 0.930340051651001
Step 324 | learning_rate: 6.59282700421941e-05
Step 324 | epoch: 1.0253164556962024
Step 325 | loss: 0.5876630544662476
Step 325 | grad_norm: 0.8453161120414734
Step 325 | learning_rate: 6.582278481012658e-05
Step 325 | epoch: 1.0284810126582278
Step 326 | loss: 0.4653762876987457
Step 326 | grad_norm: 0.8102445006370544
Step 326 | learning_rate: 6.571729957805908e-05
Step 326 | epoch: 1.0316455696202531
Step 327 | loss: 0.6289151906967163
Step 327 | grad_norm: 0.9884361028671265
Step 327 | learning_rate: 6.561181434599157e-05
Step 327 | epoch: 1.0348101265822784
Step 328 | loss: 0.6430498361587524
Step 328 | grad_norm: 1.7418824434280396
Step 328 | learning_rate: 6.550632911392405e-05
Step 328 | epoch: 1.0379746835443038
Step 329 | loss: 0.5081181526184082
Step 329 | grad_norm: 1.0529897212982178
Step 329 | learning_rate: 6.540084388185654e-05
Step 329 | epoch: 1.0411392405063291
Step 330 | loss: 0.6468367576599121
Step 330 | grad_norm: 0.9759982824325562
Step 330 | learning_rate: 6.529535864978903e-05
Step 330 | epoch: 1.0443037974683544
Step 331 | loss: 0.6777912378311157
Step 331 | grad_norm: 1.0312724113464355
Step 331 | learning_rate: 6.518987341772153e-05
Step 331 | epoch: 1.0474683544303798
Step 332 | loss: 0.6266824007034302
Step 332 | grad_norm: 0.9994598031044006
Step 332 | learning_rate: 6.5084388185654e-05
Step 332 | epoch: 1.0506329113924051
Step 333 | loss: 0.6758145689964294
Step 333 | grad_norm: 1.0371891260147095
Step 333 | learning_rate: 6.49789029535865e-05
Step 333 | epoch: 1.0537974683544304
Step 334 | loss: 0.558901309967041
Step 334 | grad_norm: 0.9417202472686768
Step 334 | learning_rate: 6.4873417721519e-05
Step 334 | epoch: 1.0569620253164558
Step 335 | loss: 0.6680985689163208
Step 335 | grad_norm: 1.0847766399383545
Step 335 | learning_rate: 6.476793248945147e-05
Step 335 | epoch: 1.0601265822784811
Step 336 | loss: 0.5823918581008911
Step 336 | grad_norm: 0.9797306060791016
Step 336 | learning_rate: 6.466244725738397e-05
Step 336 | epoch: 1.0632911392405062
Step 337 | loss: 0.533141553401947
Step 337 | grad_norm: 0.8471973538398743
Step 337 | learning_rate: 6.455696202531646e-05
Step 337 | epoch: 1.0664556962025316
Step 338 | loss: 0.6313955783843994
Step 338 | grad_norm: 1.297391414642334
Step 338 | learning_rate: 6.445147679324895e-05
Step 338 | epoch: 1.0696202531645569
Step 339 | loss: 0.5151339769363403
Step 339 | grad_norm: 0.7133660316467285
Step 339 | learning_rate: 6.434599156118144e-05
Step 339 | epoch: 1.0727848101265822
Step 340 | loss: 0.4927680492401123
Step 340 | grad_norm: 0.8298608064651489
Step 340 | learning_rate: 6.424050632911393e-05
Step 340 | epoch: 1.0759493670886076
Step 341 | loss: 0.466965913772583
Step 341 | grad_norm: 0.7823324203491211
Step 341 | learning_rate: 6.413502109704643e-05
Step 341 | epoch: 1.0791139240506329
Step 342 | loss: 0.6808106899261475
Step 342 | grad_norm: 1.05866539478302
Step 342 | learning_rate: 6.40295358649789e-05
Step 342 | epoch: 1.0822784810126582
Step 343 | loss: 0.6891579627990723
Step 343 | grad_norm: 1.2032796144485474
Step 343 | learning_rate: 6.392405063291139e-05
Step 343 | epoch: 1.0854430379746836
Step 344 | loss: 0.6014469861984253
Step 344 | grad_norm: 0.9752568602561951
Step 344 | learning_rate: 6.381856540084389e-05
Step 344 | epoch: 1.0886075949367089
Step 345 | loss: 0.5016674399375916
Step 345 | grad_norm: 1.0039310455322266
Step 345 | learning_rate: 6.371308016877638e-05
Step 345 | epoch: 1.0917721518987342
Step 346 | loss: 0.7104436755180359
Step 346 | grad_norm: 1.086430311203003
Step 346 | learning_rate: 6.360759493670885e-05
Step 346 | epoch: 1.0949367088607596
Step 347 | loss: 0.5724606513977051
Step 347 | grad_norm: 1.1757211685180664
Step 347 | learning_rate: 6.350210970464136e-05
Step 347 | epoch: 1.0981012658227849
Step 348 | loss: 0.5770403146743774
Step 348 | grad_norm: 1.0988662242889404
Step 348 | learning_rate: 6.339662447257384e-05
Step 348 | epoch: 1.1012658227848102
Step 349 | loss: 0.47078704833984375
Step 349 | grad_norm: 1.1628098487854004
Step 349 | learning_rate: 6.329113924050633e-05
Step 349 | epoch: 1.1044303797468356
Step 350 | loss: 0.5706040859222412
Step 350 | grad_norm: 1.4936046600341797
Step 350 | learning_rate: 6.318565400843882e-05
Step 350 | epoch: 1.1075949367088607
Step 351 | loss: 0.5761306881904602
Step 351 | grad_norm: 1.160146713256836
Step 351 | learning_rate: 6.308016877637131e-05
Step 351 | epoch: 1.110759493670886
Step 352 | loss: 0.6109141111373901
Step 352 | grad_norm: 1.1275073289871216
Step 352 | learning_rate: 6.29746835443038e-05
Step 352 | epoch: 1.1139240506329113
Step 353 | loss: 0.5866395235061646
Step 353 | grad_norm: 0.9119637608528137
Step 353 | learning_rate: 6.286919831223629e-05
Step 353 | epoch: 1.1170886075949367
Step 354 | loss: 0.49418389797210693
Step 354 | grad_norm: 0.9199075698852539
Step 354 | learning_rate: 6.276371308016879e-05
Step 354 | epoch: 1.120253164556962
Step 355 | loss: 0.6099327802658081
Step 355 | grad_norm: 1.113739013671875
Step 355 | learning_rate: 6.265822784810128e-05
Step 355 | epoch: 1.1234177215189873
Step 356 | loss: 0.48505908250808716
Step 356 | grad_norm: 0.8573790192604065
Step 356 | learning_rate: 6.255274261603375e-05
Step 356 | epoch: 1.1265822784810127
Step 357 | loss: 0.38783371448516846
Step 357 | grad_norm: 1.041536808013916
Step 357 | learning_rate: 6.244725738396625e-05
Step 357 | epoch: 1.129746835443038
Step 358 | loss: 0.6923028230667114
Step 358 | grad_norm: 1.2036855220794678
Step 358 | learning_rate: 6.234177215189874e-05
Step 358 | epoch: 1.1329113924050633
Step 359 | loss: 0.5549317002296448
Step 359 | grad_norm: 0.8907967209815979
Step 359 | learning_rate: 6.223628691983122e-05
Step 359 | epoch: 1.1360759493670887
Step 360 | loss: 0.5787721872329712
Step 360 | grad_norm: 1.0029445886611938
Step 360 | learning_rate: 6.213080168776372e-05
Step 360 | epoch: 1.139240506329114
Step 361 | loss: 0.5956038236618042
Step 361 | grad_norm: 1.2702662944793701
Step 361 | learning_rate: 6.20253164556962e-05
Step 361 | epoch: 1.1424050632911393
Step 362 | loss: 0.624816358089447
Step 362 | grad_norm: 1.3412177562713623
Step 362 | learning_rate: 6.191983122362869e-05
Step 362 | epoch: 1.1455696202531644
Step 363 | loss: 0.5871505737304688
Step 363 | grad_norm: 1.385546088218689
Step 363 | learning_rate: 6.181434599156118e-05
Step 363 | epoch: 1.1487341772151898
Step 364 | loss: 0.7065713405609131
Step 364 | grad_norm: 1.2230929136276245
Step 364 | learning_rate: 6.170886075949367e-05
Step 364 | epoch: 1.1518987341772151
Step 365 | loss: 0.5974803566932678
Step 365 | grad_norm: 1.1026796102523804
Step 365 | learning_rate: 6.160337552742617e-05
Step 365 | epoch: 1.1550632911392404
Step 366 | loss: 0.5754466652870178
Step 366 | grad_norm: 1.1049288511276245
Step 366 | learning_rate: 6.149789029535865e-05
Step 366 | epoch: 1.1582278481012658
Step 367 | loss: 0.6038508415222168
Step 367 | grad_norm: 1.1618080139160156
Step 367 | learning_rate: 6.139240506329115e-05
Step 367 | epoch: 1.1613924050632911
Step 368 | loss: 0.6450289487838745
Step 368 | grad_norm: 1.2799150943756104
Step 368 | learning_rate: 6.128691983122364e-05
Step 368 | epoch: 1.1645569620253164
Step 369 | loss: 0.4555737376213074
Step 369 | grad_norm: 0.9341861009597778
Step 369 | learning_rate: 6.118143459915612e-05
Step 369 | epoch: 1.1677215189873418
Step 370 | loss: 0.49384140968322754
Step 370 | grad_norm: 1.008435845375061
Step 370 | learning_rate: 6.107594936708861e-05
Step 370 | epoch: 1.1708860759493671
Step 371 | loss: 0.5897032618522644
Step 371 | grad_norm: 1.071709156036377
Step 371 | learning_rate: 6.09704641350211e-05
Step 371 | epoch: 1.1740506329113924
Step 372 | loss: 0.5615377426147461
Step 372 | grad_norm: 0.8972181677818298
Step 372 | learning_rate: 6.086497890295358e-05
Step 372 | epoch: 1.1772151898734178
Step 373 | loss: 0.6206034421920776
Step 373 | grad_norm: 1.0477908849716187
Step 373 | learning_rate: 6.0759493670886084e-05
Step 373 | epoch: 1.1803797468354431
Step 374 | loss: 0.46073564887046814
Step 374 | grad_norm: 0.9298549294471741
Step 374 | learning_rate: 6.0654008438818565e-05
Step 374 | epoch: 1.1835443037974684
Step 375 | loss: 0.5998257398605347
Step 375 | grad_norm: 1.010571837425232
Step 375 | learning_rate: 6.054852320675106e-05
Step 375 | epoch: 1.1867088607594938
Step 376 | loss: 0.5397133827209473
Step 376 | grad_norm: 1.0348999500274658
Step 376 | learning_rate: 6.044303797468355e-05
Step 376 | epoch: 1.189873417721519
Step 377 | loss: 0.5663137435913086
Step 377 | grad_norm: 1.0033236742019653
Step 377 | learning_rate: 6.033755274261603e-05
Step 377 | epoch: 1.1930379746835442
Step 378 | loss: 0.5910396575927734
Step 378 | grad_norm: 1.188340663909912
Step 378 | learning_rate: 6.023206751054853e-05
Step 378 | epoch: 1.1962025316455696
Step 379 | loss: 0.37476056814193726
Step 379 | grad_norm: 0.8905962109565735
Step 379 | learning_rate: 6.012658227848101e-05
Step 379 | epoch: 1.1993670886075949
Step 380 | loss: 0.46392977237701416
Step 380 | grad_norm: 0.8291823863983154
Step 380 | learning_rate: 6.002109704641351e-05
Step 380 | epoch: 1.2025316455696202
Step 381 | loss: 0.5987465977668762
Step 381 | grad_norm: 1.256443738937378
Step 381 | learning_rate: 5.9915611814345996e-05
Step 381 | epoch: 1.2056962025316456
Step 382 | loss: 0.5529826879501343
Step 382 | grad_norm: 0.9706960916519165
Step 382 | learning_rate: 5.981012658227848e-05
Step 382 | epoch: 1.2088607594936709
Step 383 | loss: 0.4859943985939026
Step 383 | grad_norm: 1.3844646215438843
Step 383 | learning_rate: 5.970464135021098e-05
Step 383 | epoch: 1.2120253164556962
Step 384 | loss: 0.5758013725280762
Step 384 | grad_norm: 1.205582857131958
Step 384 | learning_rate: 5.959915611814346e-05
Step 384 | epoch: 1.2151898734177216
Step 385 | loss: 0.5152626633644104
Step 385 | grad_norm: 0.9063284993171692
Step 385 | learning_rate: 5.949367088607595e-05
Step 385 | epoch: 1.2183544303797469
Step 386 | loss: 0.6388932466506958
Step 386 | grad_norm: 0.9164466857910156
Step 386 | learning_rate: 5.9388185654008444e-05
Step 386 | epoch: 1.2215189873417722
Step 387 | loss: 0.5384993553161621
Step 387 | grad_norm: 0.9256958365440369
Step 387 | learning_rate: 5.928270042194093e-05
Step 387 | epoch: 1.2246835443037976
Step 388 | loss: 0.6126603484153748
Step 388 | grad_norm: 1.5986993312835693
Step 388 | learning_rate: 5.917721518987343e-05
Step 388 | epoch: 1.2278481012658227
Step 389 | loss: 0.6005793809890747
Step 389 | grad_norm: 0.9056310057640076
Step 389 | learning_rate: 5.907172995780591e-05
Step 389 | epoch: 1.231012658227848
Step 390 | loss: 0.4684624969959259
Step 390 | grad_norm: 1.2004072666168213
Step 390 | learning_rate: 5.89662447257384e-05
Step 390 | epoch: 1.2341772151898733
Step 391 | loss: 0.5433354377746582
Step 391 | grad_norm: 0.9467605948448181
Step 391 | learning_rate: 5.886075949367089e-05
Step 391 | epoch: 1.2373417721518987
Step 392 | loss: 0.48689204454421997
Step 392 | grad_norm: 1.11344575881958
Step 392 | learning_rate: 5.875527426160338e-05
Step 392 | epoch: 1.240506329113924
Step 393 | loss: 0.5915619730949402
Step 393 | grad_norm: 1.1481232643127441
Step 393 | learning_rate: 5.8649789029535875e-05
Step 393 | epoch: 1.2436708860759493
Step 394 | loss: 0.5031684637069702
Step 394 | grad_norm: 0.9885286092758179
Step 394 | learning_rate: 5.8544303797468356e-05
Step 394 | epoch: 1.2468354430379747
Step 395 | loss: 0.6077079772949219
Step 395 | grad_norm: 1.1622915267944336
Step 395 | learning_rate: 5.8438818565400845e-05
Step 395 | epoch: 1.25
Step 396 | loss: 0.7061479091644287
Step 396 | grad_norm: 1.084207534790039
Step 396 | learning_rate: 5.833333333333334e-05
Step 396 | epoch: 1.2531645569620253
Step 397 | loss: 0.709445595741272
Step 397 | grad_norm: 1.104059100151062
Step 397 | learning_rate: 5.822784810126583e-05
Step 397 | epoch: 1.2563291139240507
Step 398 | loss: 0.5318775177001953
Step 398 | grad_norm: 1.0942275524139404
Step 398 | learning_rate: 5.812236286919831e-05
Step 398 | epoch: 1.259493670886076
Step 399 | loss: 0.5333828926086426
Step 399 | grad_norm: 1.1358023881912231
Step 399 | learning_rate: 5.8016877637130804e-05
Step 399 | epoch: 1.2626582278481013
Step 400 | loss: 0.6212785243988037
Step 400 | grad_norm: 1.1820741891860962
Step 400 | learning_rate: 5.791139240506329e-05
Step 400 | epoch: 1.2658227848101267
Step 401 | loss: 0.5204151272773743
Step 401 | grad_norm: 1.004209041595459
Step 401 | learning_rate: 5.780590717299579e-05
Step 401 | epoch: 1.268987341772152
Step 402 | loss: 0.543590247631073
Step 402 | grad_norm: 1.0093029737472534
Step 402 | learning_rate: 5.7700421940928276e-05
Step 402 | epoch: 1.2721518987341773
Step 403 | loss: 0.5793541669845581
Step 403 | grad_norm: 1.216248869895935
Step 403 | learning_rate: 5.759493670886076e-05
Step 403 | epoch: 1.2753164556962027
Step 404 | loss: 0.530566930770874
Step 404 | grad_norm: 0.9619582295417786
Step 404 | learning_rate: 5.748945147679325e-05
Step 404 | epoch: 1.2784810126582278
Step 405 | loss: 0.5182136297225952
Step 405 | grad_norm: 0.9829376935958862
Step 405 | learning_rate: 5.738396624472574e-05
Step 405 | epoch: 1.2816455696202531
Step 406 | loss: 0.6325730681419373
Step 406 | grad_norm: 1.0867712497711182
Step 406 | learning_rate: 5.7278481012658235e-05
Step 406 | epoch: 1.2848101265822784
Step 407 | loss: 0.5346335172653198
Step 407 | grad_norm: 0.9708287119865417
Step 407 | learning_rate: 5.717299578059072e-05
Step 407 | epoch: 1.2879746835443038
Step 408 | loss: 0.4539629817008972
Step 408 | grad_norm: 0.9361340999603271
Step 408 | learning_rate: 5.7067510548523205e-05
Step 408 | epoch: 1.2911392405063291
Step 409 | loss: 0.6088011860847473
Step 409 | grad_norm: 1.1718735694885254
Step 409 | learning_rate: 5.69620253164557e-05
Step 409 | epoch: 1.2943037974683544
Step 410 | loss: 0.5499545335769653
Step 410 | grad_norm: 1.081210970878601
Step 410 | learning_rate: 5.685654008438819e-05
Step 410 | epoch: 1.2974683544303798
Step 411 | loss: 0.4558008313179016
Step 411 | grad_norm: 0.8878174424171448
Step 411 | learning_rate: 5.6751054852320676e-05
Step 411 | epoch: 1.3006329113924051
Step 412 | loss: 0.6705463528633118
Step 412 | grad_norm: 0.9764496684074402
Step 412 | learning_rate: 5.664556962025317e-05
Step 412 | epoch: 1.3037974683544304
Step 413 | loss: 0.4982035756111145
Step 413 | grad_norm: 1.0521154403686523
Step 413 | learning_rate: 5.654008438818565e-05
Step 413 | epoch: 1.3069620253164558
Step 414 | loss: 0.6513583660125732
Step 414 | grad_norm: 1.389911413192749
Step 414 | learning_rate: 5.643459915611815e-05
Step 414 | epoch: 1.310126582278481
Step 415 | loss: 0.70652174949646
Step 415 | grad_norm: 1.352687954902649
Step 415 | learning_rate: 5.6329113924050636e-05
Step 415 | epoch: 1.3132911392405062
Step 416 | loss: 0.5623183250427246
Step 416 | grad_norm: 0.9101671576499939
Step 416 | learning_rate: 5.6223628691983124e-05
Step 416 | epoch: 1.3164556962025316
Step 417 | loss: 0.6431006193161011
Step 417 | grad_norm: 1.1260240077972412
Step 417 | learning_rate: 5.611814345991562e-05
Step 417 | epoch: 1.3196202531645569
Step 418 | loss: 0.509056031703949
Step 418 | grad_norm: 1.054503083229065
Step 418 | learning_rate: 5.60126582278481e-05
Step 418 | epoch: 1.3227848101265822
Step 419 | loss: 0.5793665647506714
Step 419 | grad_norm: 1.3186675310134888
Step 419 | learning_rate: 5.590717299578059e-05
Step 419 | epoch: 1.3259493670886076
Step 420 | loss: 0.6787998080253601
Step 420 | grad_norm: 1.2590593099594116
Step 420 | learning_rate: 5.5801687763713083e-05
Step 420 | epoch: 1.3291139240506329
Step 421 | loss: 0.49252599477767944
Step 421 | grad_norm: 1.3073086738586426
Step 421 | learning_rate: 5.569620253164557e-05
Step 421 | epoch: 1.3322784810126582
Step 422 | loss: 0.6546319127082825
Step 422 | grad_norm: 1.1897317171096802
Step 422 | learning_rate: 5.559071729957807e-05
Step 422 | epoch: 1.3354430379746836
Step 423 | loss: 0.5972083210945129
Step 423 | grad_norm: 1.0910121202468872
Step 423 | learning_rate: 5.548523206751055e-05
Step 423 | epoch: 1.3386075949367089
Step 424 | loss: 0.45152226090431213
Step 424 | grad_norm: 0.9023610949516296
Step 424 | learning_rate: 5.5379746835443036e-05
Step 424 | epoch: 1.3417721518987342
Step 425 | loss: 0.5445184111595154
Step 425 | grad_norm: 1.1016323566436768
Step 425 | learning_rate: 5.527426160337553e-05
Step 425 | epoch: 1.3449367088607596
Step 426 | loss: 0.5231506824493408
Step 426 | grad_norm: 1.061816692352295
Step 426 | learning_rate: 5.516877637130802e-05
Step 426 | epoch: 1.3481012658227849
Step 427 | loss: 0.5423083305358887
Step 427 | grad_norm: 0.863642692565918
Step 427 | learning_rate: 5.5063291139240514e-05
Step 427 | epoch: 1.3512658227848102
Step 428 | loss: 0.5705100297927856
Step 428 | grad_norm: 1.0971641540527344
Step 428 | learning_rate: 5.4957805907172996e-05
Step 428 | epoch: 1.3544303797468356
Step 429 | loss: 0.6329242587089539
Step 429 | grad_norm: 1.1240495443344116
Step 429 | learning_rate: 5.4852320675105484e-05
Step 429 | epoch: 1.3575949367088609
Step 430 | loss: 0.523429811000824
Step 430 | grad_norm: 1.1314386129379272
Step 430 | learning_rate: 5.474683544303798e-05
Step 430 | epoch: 1.360759493670886
Step 431 | loss: 0.49393585324287415
Step 431 | grad_norm: 0.9718184471130371
Step 431 | learning_rate: 5.464135021097047e-05
Step 431 | epoch: 1.3639240506329113
Step 432 | loss: 0.6246374845504761
Step 432 | grad_norm: 1.213694453239441
Step 432 | learning_rate: 5.453586497890295e-05
Step 432 | epoch: 1.3670886075949367
Step 433 | loss: 0.5880130529403687
Step 433 | grad_norm: 0.9849399924278259
Step 433 | learning_rate: 5.4430379746835444e-05
Step 433 | epoch: 1.370253164556962
Step 434 | loss: 0.506286084651947
Step 434 | grad_norm: 1.1581171751022339
Step 434 | learning_rate: 5.432489451476793e-05
Step 434 | epoch: 1.3734177215189873
Step 435 | loss: 0.5797519087791443
Step 435 | grad_norm: 1.0655028820037842
Step 435 | learning_rate: 5.421940928270043e-05
Step 435 | epoch: 1.3765822784810127
Step 436 | loss: 0.6142376065254211
Step 436 | grad_norm: 1.068299412727356
Step 436 | learning_rate: 5.4113924050632915e-05
Step 436 | epoch: 1.379746835443038
Step 437 | loss: 0.5729562044143677
Step 437 | grad_norm: 1.0158027410507202
Step 437 | learning_rate: 5.4008438818565396e-05
Step 437 | epoch: 1.3829113924050633
Step 438 | loss: 0.512258768081665
Step 438 | grad_norm: 1.0743927955627441
Step 438 | learning_rate: 5.39029535864979e-05
Step 438 | epoch: 1.3860759493670887
Step 439 | loss: 0.5782373547554016
Step 439 | grad_norm: 1.1567323207855225
Step 439 | learning_rate: 5.379746835443038e-05
Step 439 | epoch: 1.389240506329114
Step 440 | loss: 0.6929313540458679
Step 440 | grad_norm: 1.4843127727508545
Step 440 | learning_rate: 5.3691983122362875e-05
Step 440 | epoch: 1.3924050632911391
Step 441 | loss: 0.579765796661377
Step 441 | grad_norm: 1.1484460830688477
Step 441 | learning_rate: 5.358649789029536e-05
Step 441 | epoch: 1.3955696202531644
Step 442 | loss: 0.5986055731773376
Step 442 | grad_norm: 1.0573830604553223
Step 442 | learning_rate: 5.3481012658227844e-05
Step 442 | epoch: 1.3987341772151898
Step 443 | loss: 0.4757520854473114
Step 443 | grad_norm: 0.9754888415336609
Step 443 | learning_rate: 5.3375527426160346e-05
Step 443 | epoch: 1.4018987341772151
Step 444 | loss: 0.5121618509292603
Step 444 | grad_norm: 1.149359107017517
Step 444 | learning_rate: 5.327004219409283e-05
Step 444 | epoch: 1.4050632911392404
Step 445 | loss: 0.6128573417663574
Step 445 | grad_norm: 0.9925680756568909
Step 445 | learning_rate: 5.3164556962025316e-05
Step 445 | epoch: 1.4082278481012658
Step 446 | loss: 0.5302071571350098
Step 446 | grad_norm: 1.3358820676803589
Step 446 | learning_rate: 5.305907172995781e-05
Step 446 | epoch: 1.4113924050632911
Step 447 | loss: 0.4810901880264282
Step 447 | grad_norm: 1.1281843185424805
Step 447 | learning_rate: 5.295358649789029e-05
Step 447 | epoch: 1.4145569620253164
Step 448 | loss: 0.5872453451156616
Step 448 | grad_norm: 1.1509474515914917
Step 448 | learning_rate: 5.2848101265822794e-05
Step 448 | epoch: 1.4177215189873418
Step 449 | loss: 0.5537352561950684
Step 449 | grad_norm: 1.199561357498169
Step 449 | learning_rate: 5.2742616033755275e-05
Step 449 | epoch: 1.4208860759493671
Step 450 | loss: 0.5570651292800903
Step 450 | grad_norm: 0.9807085990905762
Step 450 | learning_rate: 5.2637130801687763e-05
Step 450 | epoch: 1.4240506329113924
Step 451 | loss: 0.5001693367958069
Step 451 | grad_norm: 0.915160596370697
Step 451 | learning_rate: 5.253164556962026e-05
Step 451 | epoch: 1.4272151898734178
Step 452 | loss: 0.6436772346496582
Step 452 | grad_norm: 1.3140448331832886
Step 452 | learning_rate: 5.242616033755275e-05
Step 452 | epoch: 1.4303797468354431
Step 453 | loss: 0.5264090895652771
Step 453 | grad_norm: 1.0504968166351318
Step 453 | learning_rate: 5.232067510548524e-05
Step 453 | epoch: 1.4335443037974684
Step 454 | loss: 0.7739061713218689
Step 454 | grad_norm: 1.3831740617752075
Step 454 | learning_rate: 5.221518987341772e-05
Step 454 | epoch: 1.4367088607594938
Step 455 | loss: 0.5629180669784546
Step 455 | grad_norm: 0.954176664352417
Step 455 | learning_rate: 5.210970464135021e-05
Step 455 | epoch: 1.439873417721519
Step 456 | loss: 0.49939897656440735
Step 456 | grad_norm: 1.013740062713623
Step 456 | learning_rate: 5.2004219409282706e-05
Step 456 | epoch: 1.4430379746835442
Step 457 | loss: 0.4808853268623352
Step 457 | grad_norm: 1.016319990158081
Step 457 | learning_rate: 5.1898734177215194e-05
Step 457 | epoch: 1.4462025316455696
Step 458 | loss: 0.5669866800308228
Step 458 | grad_norm: 0.9361384510993958
Step 458 | learning_rate: 5.1793248945147676e-05
Step 458 | epoch: 1.4493670886075949
Step 459 | loss: 0.5302478671073914
Step 459 | grad_norm: 1.2705886363983154
Step 459 | learning_rate: 5.168776371308017e-05
Step 459 | epoch: 1.4525316455696202
Step 460 | loss: 0.46806657314300537
Step 460 | grad_norm: 0.9294326901435852
Step 460 | learning_rate: 5.158227848101266e-05
Step 460 | epoch: 1.4556962025316456
Step 461 | loss: 0.5544451475143433
Step 461 | grad_norm: 0.9844509959220886
Step 461 | learning_rate: 5.1476793248945154e-05
Step 461 | epoch: 1.4588607594936709
Step 462 | loss: 0.42196550965309143
Step 462 | grad_norm: 1.4191914796829224
Step 462 | learning_rate: 5.137130801687764e-05
Step 462 | epoch: 1.4620253164556962
Step 463 | loss: 0.5230673551559448
Step 463 | grad_norm: 1.208855152130127
Step 463 | learning_rate: 5.1265822784810124e-05
Step 463 | epoch: 1.4651898734177216
Step 464 | loss: 0.5897612571716309
Step 464 | grad_norm: 1.238511562347412
Step 464 | learning_rate: 5.116033755274262e-05
Step 464 | epoch: 1.4683544303797469
Step 465 | loss: 0.5867414474487305
Step 465 | grad_norm: 0.9310600161552429
Step 465 | learning_rate: 5.105485232067511e-05
Step 465 | epoch: 1.4715189873417722
Step 466 | loss: 0.42672911286354065
Step 466 | grad_norm: 1.3486965894699097
Step 466 | learning_rate: 5.09493670886076e-05
Step 466 | epoch: 1.4746835443037973
Step 467 | loss: 0.6318008303642273
Step 467 | grad_norm: 1.2398136854171753
Step 467 | learning_rate: 5.084388185654009e-05
Step 467 | epoch: 1.4778481012658227
Step 468 | loss: 0.5181045532226562
Step 468 | grad_norm: 1.0289602279663086
Step 468 | learning_rate: 5.073839662447257e-05
Step 468 | epoch: 1.481012658227848
Step 469 | loss: 0.48688018321990967
Step 469 | grad_norm: 1.5901334285736084
Step 469 | learning_rate: 5.0632911392405066e-05
Step 469 | epoch: 1.4841772151898733
Step 470 | loss: 0.7202481627464294
Step 470 | grad_norm: 1.376591444015503
Step 470 | learning_rate: 5.0527426160337555e-05
Step 470 | epoch: 1.4873417721518987
Step 471 | loss: 0.5732060670852661
Step 471 | grad_norm: 1.0049920082092285
Step 471 | learning_rate: 5.042194092827004e-05
Step 471 | epoch: 1.490506329113924
Step 472 | loss: 0.5158746242523193
Step 472 | grad_norm: 1.0673043727874756
Step 472 | learning_rate: 5.031645569620254e-05
Step 472 | epoch: 1.4936708860759493
Step 473 | loss: 0.7836518287658691
Step 473 | grad_norm: 1.1907774209976196
Step 473 | learning_rate: 5.021097046413502e-05
Step 473 | epoch: 1.4968354430379747
Step 474 | loss: 0.4299083650112152
Step 474 | grad_norm: 0.9294971823692322
Step 474 | learning_rate: 5.0105485232067514e-05
Step 474 | epoch: 1.5
Step 475 | loss: 0.48807793855667114
Step 475 | grad_norm: 1.139248013496399
Step 475 | learning_rate: 5e-05
Step 475 | epoch: 1.5031645569620253
Step 476 | loss: 0.5184798240661621
Step 476 | grad_norm: 1.070095419883728
Step 476 | learning_rate: 4.989451476793249e-05
Step 476 | epoch: 1.5063291139240507
Step 477 | loss: 0.40753859281539917
Step 477 | grad_norm: 1.1633108854293823
Step 477 | learning_rate: 4.9789029535864986e-05
Step 477 | epoch: 1.509493670886076
Step 478 | loss: 0.5594918727874756
Step 478 | grad_norm: 0.9066515564918518
Step 478 | learning_rate: 4.968354430379747e-05
Step 478 | epoch: 1.5126582278481013
Step 479 | loss: 0.5279846787452698
Step 479 | grad_norm: 0.914229691028595
Step 479 | learning_rate: 4.957805907172996e-05
Step 479 | epoch: 1.5158227848101267
Step 480 | loss: 0.533236563205719
Step 480 | grad_norm: 1.0909727811813354
Step 480 | learning_rate: 4.947257383966245e-05
Step 480 | epoch: 1.518987341772152
Step 481 | loss: 0.6221480965614319
Step 481 | grad_norm: 1.1467221975326538
Step 481 | learning_rate: 4.936708860759494e-05
Step 481 | epoch: 1.5221518987341773
Step 482 | loss: 0.6252148151397705
Step 482 | grad_norm: 1.4593167304992676
Step 482 | learning_rate: 4.9261603375527427e-05
Step 482 | epoch: 1.5253164556962027
Step 483 | loss: 0.5695935487747192
Step 483 | grad_norm: 2.1705729961395264
Step 483 | learning_rate: 4.9156118143459915e-05
Step 483 | epoch: 1.528481012658228
Step 484 | loss: 0.5162747502326965
Step 484 | grad_norm: 0.8617767095565796
Step 484 | learning_rate: 4.905063291139241e-05
Step 484 | epoch: 1.5316455696202531
Step 485 | loss: 0.5922174453735352
Step 485 | grad_norm: 1.3878270387649536
Step 485 | learning_rate: 4.89451476793249e-05
Step 485 | epoch: 1.5348101265822784
Step 486 | loss: 0.6397002339363098
Step 486 | grad_norm: 1.010990023612976
Step 486 | learning_rate: 4.8839662447257386e-05
Step 486 | epoch: 1.5379746835443038
Step 487 | loss: 0.5285266041755676
Step 487 | grad_norm: 0.9775648713111877
Step 487 | learning_rate: 4.8734177215189874e-05
Step 487 | epoch: 1.5411392405063291
Step 488 | loss: 0.49350517988204956
Step 488 | grad_norm: 0.9382921457290649
Step 488 | learning_rate: 4.862869198312236e-05
Step 488 | epoch: 1.5443037974683544
Step 489 | loss: 0.5195425748825073
Step 489 | grad_norm: 0.960379421710968
Step 489 | learning_rate: 4.852320675105486e-05
Step 489 | epoch: 1.5474683544303798
Step 490 | loss: 0.640811026096344
Step 490 | grad_norm: 0.9750969409942627
Step 490 | learning_rate: 4.8417721518987346e-05
Step 490 | epoch: 1.5506329113924051
Step 491 | loss: 0.44272294640541077
Step 491 | grad_norm: 1.1507281064987183
Step 491 | learning_rate: 4.8312236286919834e-05
Step 491 | epoch: 1.5537974683544302
Step 492 | loss: 0.5594521164894104
Step 492 | grad_norm: 0.9837843775749207
Step 492 | learning_rate: 4.820675105485232e-05
Step 492 | epoch: 1.5569620253164556
Step 493 | loss: 0.5578770637512207
Step 493 | grad_norm: 0.838524341583252
Step 493 | learning_rate: 4.810126582278481e-05
Step 493 | epoch: 1.560126582278481
Step 494 | loss: 0.4924023151397705
Step 494 | grad_norm: 1.1047676801681519
Step 494 | learning_rate: 4.7995780590717305e-05
Step 494 | epoch: 1.5632911392405062
Step 495 | loss: 0.7426071166992188
Step 495 | grad_norm: 1.3354837894439697
Step 495 | learning_rate: 4.789029535864979e-05
Step 495 | epoch: 1.5664556962025316
Step 496 | loss: 0.6062231063842773
Step 496 | grad_norm: 1.319481372833252
Step 496 | learning_rate: 4.778481012658228e-05
Step 496 | epoch: 1.5696202531645569
Step 497 | loss: 0.6380516290664673
Step 497 | grad_norm: 1.1195675134658813
Step 497 | learning_rate: 4.767932489451477e-05
Step 497 | epoch: 1.5727848101265822
Step 498 | loss: 0.5201640725135803
Step 498 | grad_norm: 1.0907304286956787
Step 498 | learning_rate: 4.757383966244726e-05
Step 498 | epoch: 1.5759493670886076
Step 499 | loss: 0.583620548248291
Step 499 | grad_norm: 1.2958812713623047
Step 499 | learning_rate: 4.7468354430379746e-05
Step 499 | epoch: 1.5791139240506329
Step 500 | loss: 0.5006235837936401
Step 500 | grad_norm: 0.9382302165031433
Step 500 | learning_rate: 4.7362869198312235e-05
Step 500 | epoch: 1.5822784810126582
Step 501 | loss: 0.5783582329750061
Step 501 | grad_norm: 1.146764874458313
Step 501 | learning_rate: 4.725738396624473e-05
Step 501 | epoch: 1.5854430379746836
Step 502 | loss: 0.602432906627655
Step 502 | grad_norm: 1.6382126808166504
Step 502 | learning_rate: 4.715189873417722e-05
Step 502 | epoch: 1.5886075949367089
Step 503 | loss: 0.6064789295196533
Step 503 | grad_norm: 1.4230014085769653
Step 503 | learning_rate: 4.704641350210971e-05
Step 503 | epoch: 1.5917721518987342
Step 504 | loss: 0.6457184553146362
Step 504 | grad_norm: 1.276711344718933
Step 504 | learning_rate: 4.6940928270042194e-05
Step 504 | epoch: 1.5949367088607596
Step 505 | loss: 0.5323183536529541
Step 505 | grad_norm: 0.9953334927558899
Step 505 | learning_rate: 4.683544303797468e-05
Step 505 | epoch: 1.5981012658227849
Step 506 | loss: 0.5414876937866211
Step 506 | grad_norm: 1.1166281700134277
Step 506 | learning_rate: 4.672995780590718e-05
Step 506 | epoch: 1.6012658227848102
Step 507 | loss: 0.5310199856758118
Step 507 | grad_norm: 1.2292840480804443
Step 507 | learning_rate: 4.6624472573839666e-05
Step 507 | epoch: 1.6044303797468356
Step 508 | loss: 0.4664555788040161
Step 508 | grad_norm: 1.1709787845611572
Step 508 | learning_rate: 4.6518987341772154e-05
Step 508 | epoch: 1.6075949367088609
Step 509 | loss: 0.5708103179931641
Step 509 | grad_norm: 1.2997392416000366
Step 509 | learning_rate: 4.641350210970464e-05
Step 509 | epoch: 1.6107594936708862
Step 510 | loss: 0.5121573209762573
Step 510 | grad_norm: 1.2507319450378418
Step 510 | learning_rate: 4.630801687763714e-05
Step 510 | epoch: 1.6139240506329116
Step 511 | loss: 0.5522017478942871
Step 511 | grad_norm: 1.2359012365341187
Step 511 | learning_rate: 4.6202531645569625e-05
Step 511 | epoch: 1.6170886075949367
Step 512 | loss: 0.6354244947433472
Step 512 | grad_norm: 1.373477578163147
Step 512 | learning_rate: 4.6097046413502107e-05
Step 512 | epoch: 1.620253164556962
Step 513 | loss: 0.5460506081581116
Step 513 | grad_norm: 1.1968443393707275
Step 513 | learning_rate: 4.59915611814346e-05
Step 513 | epoch: 1.6234177215189873
Step 514 | loss: 0.6015492677688599
Step 514 | grad_norm: 1.195785403251648
Step 514 | learning_rate: 4.588607594936709e-05
Step 514 | epoch: 1.6265822784810127
Step 515 | loss: 0.5538028478622437
Step 515 | grad_norm: 1.0985078811645508
Step 515 | learning_rate: 4.5780590717299585e-05
Step 515 | epoch: 1.629746835443038
Step 516 | loss: 0.5607147812843323
Step 516 | grad_norm: 1.1490108966827393
Step 516 | learning_rate: 4.5675105485232066e-05
Step 516 | epoch: 1.6329113924050633
Step 517 | loss: 0.6213786005973816
Step 517 | grad_norm: 1.1183782815933228
Step 517 | learning_rate: 4.556962025316456e-05
Step 517 | epoch: 1.6360759493670884
Step 518 | loss: 0.49608975648880005
Step 518 | grad_norm: 1.0855906009674072
Step 518 | learning_rate: 4.546413502109705e-05
Step 518 | epoch: 1.6392405063291138
Step 519 | loss: 0.5700676441192627
Step 519 | grad_norm: 1.288543462753296
Step 519 | learning_rate: 4.535864978902954e-05
Step 519 | epoch: 1.6424050632911391
Step 520 | loss: 0.49137306213378906
Step 520 | grad_norm: 0.9861487746238708
Step 520 | learning_rate: 4.525316455696203e-05
Step 520 | epoch: 1.6455696202531644
Step 521 | loss: 0.6853772401809692
Step 521 | grad_norm: 1.0153712034225464
Step 521 | learning_rate: 4.5147679324894514e-05
Step 521 | epoch: 1.6487341772151898
Step 522 | loss: 0.4855774939060211
Step 522 | grad_norm: 1.0467463731765747
Step 522 | learning_rate: 4.504219409282701e-05
Step 522 | epoch: 1.6518987341772151
Step 523 | loss: 0.4476463496685028
Step 523 | grad_norm: 1.170361042022705
Step 523 | learning_rate: 4.49367088607595e-05
Step 523 | epoch: 1.6550632911392404
Step 524 | loss: 0.4956165850162506
Step 524 | grad_norm: 0.8848394155502319
Step 524 | learning_rate: 4.4831223628691985e-05
Step 524 | epoch: 1.6582278481012658
Step 525 | loss: 0.49805641174316406
Step 525 | grad_norm: 1.0888831615447998
Step 525 | learning_rate: 4.4725738396624474e-05
Step 525 | epoch: 1.6613924050632911
Step 526 | loss: 0.6236206889152527
Step 526 | grad_norm: 1.03413724899292
Step 526 | learning_rate: 4.462025316455696e-05
Step 526 | epoch: 1.6645569620253164
Step 527 | loss: 0.590668797492981
Step 527 | grad_norm: 1.241925835609436
Step 527 | learning_rate: 4.451476793248946e-05
Step 527 | epoch: 1.6677215189873418
Step 528 | loss: 0.46320173144340515
Step 528 | grad_norm: 1.1605132818222046
Step 528 | learning_rate: 4.4409282700421945e-05
Step 528 | epoch: 1.6708860759493671
Step 529 | loss: 0.7673852443695068
Step 529 | grad_norm: 1.7268145084381104
Step 529 | learning_rate: 4.430379746835443e-05
Step 529 | epoch: 1.6740506329113924
Step 530 | loss: 0.6779837608337402
Step 530 | grad_norm: 1.2811024188995361
Step 530 | learning_rate: 4.419831223628692e-05
Step 530 | epoch: 1.6772151898734178
Step 531 | loss: 0.4516397714614868
Step 531 | grad_norm: 1.351499319076538
Step 531 | learning_rate: 4.409282700421941e-05
Step 531 | epoch: 1.6803797468354431
Step 532 | loss: 0.3688713312149048
Step 532 | grad_norm: 0.7354739308357239
Step 532 | learning_rate: 4.3987341772151904e-05
Step 532 | epoch: 1.6835443037974684
Step 533 | loss: 0.5792299509048462
Step 533 | grad_norm: 1.1767245531082153
Step 533 | learning_rate: 4.388185654008439e-05
Step 533 | epoch: 1.6867088607594938
Step 534 | loss: 0.46367400884628296
Step 534 | grad_norm: 0.9416221380233765
Step 534 | learning_rate: 4.377637130801688e-05
Step 534 | epoch: 1.689873417721519
Step 535 | loss: 0.6092197299003601
Step 535 | grad_norm: 1.319771647453308
Step 535 | learning_rate: 4.367088607594937e-05
Step 535 | epoch: 1.6930379746835444
Step 536 | loss: 0.549766480922699
Step 536 | grad_norm: 1.2496414184570312
Step 536 | learning_rate: 4.356540084388186e-05
Step 536 | epoch: 1.6962025316455698
Step 537 | loss: 0.5194604992866516
Step 537 | grad_norm: 0.9603099226951599
Step 537 | learning_rate: 4.345991561181435e-05
Step 537 | epoch: 1.6993670886075949
Step 538 | loss: 0.5317497253417969
Step 538 | grad_norm: 1.0374315977096558
Step 538 | learning_rate: 4.3354430379746834e-05
Step 538 | epoch: 1.7025316455696202
Step 539 | loss: 0.5458920001983643
Step 539 | grad_norm: 1.1564863920211792
Step 539 | learning_rate: 4.324894514767933e-05
Step 539 | epoch: 1.7056962025316456
Step 540 | loss: 0.6254198551177979
Step 540 | grad_norm: 1.1958227157592773
Step 540 | learning_rate: 4.314345991561182e-05
Step 540 | epoch: 1.7088607594936709
Step 541 | loss: 0.6208211183547974
Step 541 | grad_norm: 1.0729984045028687
Step 541 | learning_rate: 4.3037974683544305e-05
Step 541 | epoch: 1.7120253164556962
Step 542 | loss: 0.6210231781005859
Step 542 | grad_norm: 1.2998003959655762
Step 542 | learning_rate: 4.293248945147679e-05
Step 542 | epoch: 1.7151898734177216
Step 543 | loss: 0.6286334991455078
Step 543 | grad_norm: 1.2966337203979492
Step 543 | learning_rate: 4.282700421940928e-05
Step 543 | epoch: 1.7183544303797469
Step 544 | loss: 0.5644505023956299
Step 544 | grad_norm: 1.3396819829940796
Step 544 | learning_rate: 4.2721518987341776e-05
Step 544 | epoch: 1.721518987341772
Step 545 | loss: 0.5113523006439209
Step 545 | grad_norm: 1.027590036392212
Step 545 | learning_rate: 4.2616033755274265e-05
Step 545 | epoch: 1.7246835443037973
Step 546 | loss: 0.5245188474655151
Step 546 | grad_norm: 1.035889744758606
Step 546 | learning_rate: 4.251054852320675e-05
Step 546 | epoch: 1.7278481012658227
Step 547 | loss: 0.6558733582496643
Step 547 | grad_norm: 1.0364288091659546
Step 547 | learning_rate: 4.240506329113924e-05
Step 547 | epoch: 1.731012658227848
Step 548 | loss: 0.5836162567138672
Step 548 | grad_norm: 1.0517547130584717
Step 548 | learning_rate: 4.229957805907173e-05
Step 548 | epoch: 1.7341772151898733
Step 549 | loss: 0.5263887643814087
Step 549 | grad_norm: 0.9566953182220459
Step 549 | learning_rate: 4.2194092827004224e-05
Step 549 | epoch: 1.7373417721518987
Step 550 | loss: 0.5599828958511353
Step 550 | grad_norm: 1.0690875053405762
Step 550 | learning_rate: 4.208860759493671e-05
Step 550 | epoch: 1.740506329113924
Step 551 | loss: 0.5349472761154175
Step 551 | grad_norm: 1.4776777029037476
Step 551 | learning_rate: 4.19831223628692e-05
Step 551 | epoch: 1.7436708860759493
Step 552 | loss: 0.6199337244033813
Step 552 | grad_norm: 1.2120939493179321
Step 552 | learning_rate: 4.187763713080169e-05
Step 552 | epoch: 1.7468354430379747
Step 553 | loss: 0.5952449440956116
Step 553 | grad_norm: 1.034692645072937
Step 553 | learning_rate: 4.177215189873418e-05
Step 553 | epoch: 1.75
Step 554 | loss: 0.6038584113121033
Step 554 | grad_norm: 1.2128493785858154
Step 554 | learning_rate: 4.166666666666667e-05
Step 554 | epoch: 1.7531645569620253
Step 555 | loss: 0.6323707103729248
Step 555 | grad_norm: 1.3175104856491089
Step 555 | learning_rate: 4.1561181434599153e-05
Step 555 | epoch: 1.7563291139240507
Step 556 | loss: 0.4702944755554199
Step 556 | grad_norm: 0.9091799855232239
Step 556 | learning_rate: 4.145569620253165e-05
Step 556 | epoch: 1.759493670886076
Step 557 | loss: 0.6388009786605835
Step 557 | grad_norm: 1.0643351078033447
Step 557 | learning_rate: 4.135021097046414e-05
Step 557 | epoch: 1.7626582278481013
Step 558 | loss: 0.5888631343841553
Step 558 | grad_norm: 1.076187014579773
Step 558 | learning_rate: 4.1244725738396625e-05
Step 558 | epoch: 1.7658227848101267
Step 559 | loss: 0.49294742941856384
Step 559 | grad_norm: 0.9544225335121155
Step 559 | learning_rate: 4.113924050632912e-05
Step 559 | epoch: 1.768987341772152
Step 560 | loss: 0.6002150177955627
Step 560 | grad_norm: 1.2061042785644531
Step 560 | learning_rate: 4.10337552742616e-05
Step 560 | epoch: 1.7721518987341773
Step 561 | loss: 0.45763343572616577
Step 561 | grad_norm: 1.4187861680984497
Step 561 | learning_rate: 4.0928270042194096e-05
Step 561 | epoch: 1.7753164556962027
Step 562 | loss: 0.5849618315696716
Step 562 | grad_norm: 1.0704267024993896
Step 562 | learning_rate: 4.0822784810126584e-05
Step 562 | epoch: 1.778481012658228
Step 563 | loss: 0.49859386682510376
Step 563 | grad_norm: 1.221332311630249
Step 563 | learning_rate: 4.071729957805907e-05
Step 563 | epoch: 1.7816455696202531
Step 564 | loss: 0.5813462734222412
Step 564 | grad_norm: 1.328028917312622
Step 564 | learning_rate: 4.061181434599156e-05
Step 564 | epoch: 1.7848101265822784
Step 565 | loss: 0.5648402571678162
Step 565 | grad_norm: 1.0705735683441162
Step 565 | learning_rate: 4.050632911392405e-05
Step 565 | epoch: 1.7879746835443038
Step 566 | loss: 0.5137690901756287
Step 566 | grad_norm: 1.1464025974273682
Step 566 | learning_rate: 4.0400843881856544e-05
Step 566 | epoch: 1.7911392405063291
Step 567 | loss: 0.6925533413887024
Step 567 | grad_norm: 1.4299471378326416
Step 567 | learning_rate: 4.029535864978903e-05
Step 567 | epoch: 1.7943037974683544
Step 568 | loss: 0.6327807903289795
Step 568 | grad_norm: 1.175949215888977
Step 568 | learning_rate: 4.018987341772152e-05
Step 568 | epoch: 1.7974683544303798
Step 569 | loss: 0.5723373293876648
Step 569 | grad_norm: 1.0907399654388428
Step 569 | learning_rate: 4.008438818565401e-05
Step 569 | epoch: 1.8006329113924051
Step 570 | loss: 0.5806334018707275
Step 570 | grad_norm: 0.9517611265182495
Step 570 | learning_rate: 3.99789029535865e-05
Step 570 | epoch: 1.8037974683544302
Step 571 | loss: 0.5616472959518433
Step 571 | grad_norm: 1.3043540716171265
Step 571 | learning_rate: 3.987341772151899e-05
Step 571 | epoch: 1.8069620253164556
Step 572 | loss: 0.4681662321090698
Step 572 | grad_norm: 1.089956283569336
Step 572 | learning_rate: 3.976793248945147e-05
Step 572 | epoch: 1.810126582278481
Step 573 | loss: 0.4648246169090271
Step 573 | grad_norm: 0.9402133822441101
Step 573 | learning_rate: 3.966244725738397e-05
Step 573 | epoch: 1.8132911392405062
Step 574 | loss: 0.5558040142059326
Step 574 | grad_norm: 1.0004048347473145
Step 574 | learning_rate: 3.9556962025316456e-05
Step 574 | epoch: 1.8164556962025316
Step 575 | loss: 0.5793084502220154
Step 575 | grad_norm: 1.0988869667053223
Step 575 | learning_rate: 3.945147679324895e-05
Step 575 | epoch: 1.8196202531645569
Step 576 | loss: 0.6550156474113464
Step 576 | grad_norm: 1.3161795139312744
Step 576 | learning_rate: 3.934599156118144e-05
Step 576 | epoch: 1.8227848101265822
Step 577 | loss: 0.5337820649147034
Step 577 | grad_norm: 1.1183041334152222
Step 577 | learning_rate: 3.924050632911392e-05
Step 577 | epoch: 1.8259493670886076
Step 578 | loss: 0.6344516277313232
Step 578 | grad_norm: 1.2688273191452026
Step 578 | learning_rate: 3.9135021097046416e-05
Step 578 | epoch: 1.8291139240506329
Step 579 | loss: 0.5882788896560669
Step 579 | grad_norm: 1.0858772993087769
Step 579 | learning_rate: 3.9029535864978904e-05
Step 579 | epoch: 1.8322784810126582
Step 580 | loss: 0.5574328899383545
Step 580 | grad_norm: 1.1123663187026978
Step 580 | learning_rate: 3.89240506329114e-05
Step 580 | epoch: 1.8354430379746836
Step 581 | loss: 0.4396795928478241
Step 581 | grad_norm: 0.9494760632514954
Step 581 | learning_rate: 3.881856540084388e-05
Step 581 | epoch: 1.8386075949367089
Step 582 | loss: 0.5465943217277527
Step 582 | grad_norm: 1.1457602977752686
Step 582 | learning_rate: 3.8713080168776376e-05
Step 582 | epoch: 1.8417721518987342
Step 583 | loss: 0.3898807168006897
Step 583 | grad_norm: 0.8485133051872253
Step 583 | learning_rate: 3.8607594936708864e-05
Step 583 | epoch: 1.8449367088607596
Step 584 | loss: 0.5309749245643616
Step 584 | grad_norm: 1.099297285079956
Step 584 | learning_rate: 3.850210970464135e-05
Step 584 | epoch: 1.8481012658227849
Step 585 | loss: 0.4697141945362091
Step 585 | grad_norm: 0.8771138787269592
Step 585 | learning_rate: 3.839662447257384e-05
Step 585 | epoch: 1.8512658227848102
Step 586 | loss: 0.5657150745391846
Step 586 | grad_norm: 1.077562689781189
Step 586 | learning_rate: 3.829113924050633e-05
Step 586 | epoch: 1.8544303797468356
Step 587 | loss: 0.5427897572517395
Step 587 | grad_norm: 1.266358494758606
Step 587 | learning_rate: 3.8185654008438823e-05
Step 587 | epoch: 1.8575949367088609
Step 588 | loss: 0.5692775249481201
Step 588 | grad_norm: 1.2538667917251587
Step 588 | learning_rate: 3.808016877637131e-05
Step 588 | epoch: 1.8607594936708862
Step 589 | loss: 0.5731174349784851
Step 589 | grad_norm: 1.0184768438339233
Step 589 | learning_rate: 3.79746835443038e-05
Step 589 | epoch: 1.8639240506329116
Step 590 | loss: 0.6268019080162048
Step 590 | grad_norm: 1.2596735954284668
Step 590 | learning_rate: 3.786919831223629e-05
Step 590 | epoch: 1.8670886075949367
Step 591 | loss: 0.558689534664154
Step 591 | grad_norm: 1.0905990600585938
Step 591 | learning_rate: 3.7763713080168776e-05
Step 591 | epoch: 1.870253164556962
Step 592 | loss: 0.6068581938743591
Step 592 | grad_norm: 1.0474940538406372
Step 592 | learning_rate: 3.765822784810127e-05
Step 592 | epoch: 1.8734177215189873
Step 593 | loss: 0.5087181329727173
Step 593 | grad_norm: 1.1304627656936646
Step 593 | learning_rate: 3.755274261603376e-05
Step 593 | epoch: 1.8765822784810127
Step 594 | loss: 0.5277848243713379
Step 594 | grad_norm: 0.9209833741188049
Step 594 | learning_rate: 3.744725738396625e-05
Step 594 | epoch: 1.879746835443038
Step 595 | loss: 0.6386529207229614
Step 595 | grad_norm: 1.221335530281067
Step 595 | learning_rate: 3.7341772151898736e-05
Step 595 | epoch: 1.8829113924050633
Step 596 | loss: 0.571062445640564
Step 596 | grad_norm: 1.0478867292404175
Step 596 | learning_rate: 3.7236286919831224e-05
Step 596 | epoch: 1.8860759493670884
Step 597 | loss: 0.5657614469528198
Step 597 | grad_norm: 1.3711116313934326
Step 597 | learning_rate: 3.713080168776372e-05
Step 597 | epoch: 1.8892405063291138
Step 598 | loss: 0.5523014664649963
Step 598 | grad_norm: 1.0733283758163452
Step 598 | learning_rate: 3.70253164556962e-05
Step 598 | epoch: 1.8924050632911391
Step 599 | loss: 0.6675691604614258
Step 599 | grad_norm: 1.2838200330734253
Step 599 | learning_rate: 3.6919831223628695e-05
Step 599 | epoch: 1.8955696202531644
Step 600 | loss: 0.6260209083557129
Step 600 | grad_norm: 1.217343807220459
Step 600 | learning_rate: 3.6814345991561184e-05
Step 600 | epoch: 1.8987341772151898
Step 601 | loss: 0.4547956883907318
Step 601 | grad_norm: 0.9790632724761963
Step 601 | learning_rate: 3.670886075949367e-05
Step 601 | epoch: 1.9018987341772151
Step 602 | loss: 0.503923237323761
Step 602 | grad_norm: 0.9155523180961609
Step 602 | learning_rate: 3.660337552742617e-05
Step 602 | epoch: 1.9050632911392404
Step 603 | loss: 0.4977664053440094
Step 603 | grad_norm: 0.87754887342453
Step 603 | learning_rate: 3.649789029535865e-05
Step 603 | epoch: 1.9082278481012658
Step 604 | loss: 0.4879451096057892
Step 604 | grad_norm: 0.907617449760437
Step 604 | learning_rate: 3.639240506329114e-05
Step 604 | epoch: 1.9113924050632911
Step 605 | loss: 0.5086089372634888
Step 605 | grad_norm: 0.9885414838790894
Step 605 | learning_rate: 3.628691983122363e-05
Step 605 | epoch: 1.9145569620253164
Step 606 | loss: 0.48281413316726685
Step 606 | grad_norm: 1.0343631505966187
Step 606 | learning_rate: 3.618143459915612e-05
Step 606 | epoch: 1.9177215189873418
Step 607 | loss: 0.5884355902671814
Step 607 | grad_norm: 1.1717807054519653
Step 607 | learning_rate: 3.607594936708861e-05
Step 607 | epoch: 1.9208860759493671
Step 608 | loss: 0.5134474039077759
Step 608 | grad_norm: 0.9725759029388428
Step 608 | learning_rate: 3.5970464135021096e-05
Step 608 | epoch: 1.9240506329113924
Step 609 | loss: 0.5418055057525635
Step 609 | grad_norm: 0.9122585654258728
Step 609 | learning_rate: 3.586497890295359e-05
Step 609 | epoch: 1.9272151898734178
Step 610 | loss: 0.4833008050918579
Step 610 | grad_norm: 1.0051884651184082
Step 610 | learning_rate: 3.575949367088608e-05
Step 610 | epoch: 1.9303797468354431
Step 611 | loss: 0.5413937568664551
Step 611 | grad_norm: 1.1175509691238403
Step 611 | learning_rate: 3.565400843881857e-05
Step 611 | epoch: 1.9335443037974684
Step 612 | loss: 0.5642213225364685
Step 612 | grad_norm: 1.2854901552200317
Step 612 | learning_rate: 3.5548523206751056e-05
Step 612 | epoch: 1.9367088607594938
Step 613 | loss: 0.546780526638031
Step 613 | grad_norm: 1.2912425994873047
Step 613 | learning_rate: 3.5443037974683544e-05
Step 613 | epoch: 1.939873417721519
Step 614 | loss: 0.6682755351066589
Step 614 | grad_norm: 1.2468571662902832
Step 614 | learning_rate: 3.533755274261604e-05
Step 614 | epoch: 1.9430379746835444
Step 615 | loss: 0.529208242893219
Step 615 | grad_norm: 1.1325489282608032
Step 615 | learning_rate: 3.523206751054853e-05
Step 615 | epoch: 1.9462025316455698
Step 616 | loss: 0.5841612815856934
Step 616 | grad_norm: 1.0742998123168945
Step 616 | learning_rate: 3.5126582278481015e-05
Step 616 | epoch: 1.9493670886075949
Step 617 | loss: 0.4743761122226715
Step 617 | grad_norm: 1.2546502351760864
Step 617 | learning_rate: 3.50210970464135e-05
Step 617 | epoch: 1.9525316455696202
Step 618 | loss: 0.5723327994346619
Step 618 | grad_norm: 1.0600250959396362
Step 618 | learning_rate: 3.491561181434599e-05
Step 618 | epoch: 1.9556962025316456
Step 619 | loss: 0.4739248752593994
Step 619 | grad_norm: 1.0825519561767578
Step 619 | learning_rate: 3.4810126582278487e-05
Step 619 | epoch: 1.9588607594936709
Step 620 | loss: 0.5848259925842285
Step 620 | grad_norm: 1.0944100618362427
Step 620 | learning_rate: 3.470464135021097e-05
Step 620 | epoch: 1.9620253164556962
Step 621 | loss: 0.6707991361618042
Step 621 | grad_norm: 1.2772146463394165
Step 621 | learning_rate: 3.459915611814346e-05
Step 621 | epoch: 1.9651898734177216
Step 622 | loss: 0.7032016515731812
Step 622 | grad_norm: 1.2892204523086548
Step 622 | learning_rate: 3.449367088607595e-05
Step 622 | epoch: 1.9683544303797469
Step 623 | loss: 0.6790584325790405
Step 623 | grad_norm: 1.2181298732757568
Step 623 | learning_rate: 3.438818565400844e-05
Step 623 | epoch: 1.971518987341772
Step 624 | loss: 0.4697086811065674
Step 624 | grad_norm: 1.1205830574035645
Step 624 | learning_rate: 3.428270042194093e-05
Step 624 | epoch: 1.9746835443037973
Step 625 | loss: 0.5125247836112976
Step 625 | grad_norm: 1.221264362335205
Step 625 | learning_rate: 3.4177215189873416e-05
Step 625 | epoch: 1.9778481012658227
Step 626 | loss: 0.7338157892227173
Step 626 | grad_norm: 1.2783153057098389
Step 626 | learning_rate: 3.407172995780591e-05
Step 626 | epoch: 1.981012658227848
Step 627 | loss: 0.4900926649570465
Step 627 | grad_norm: 1.09733247756958
Step 627 | learning_rate: 3.39662447257384e-05
Step 627 | epoch: 1.9841772151898733
Step 628 | loss: 0.7040388584136963
Step 628 | grad_norm: 1.246947169303894
Step 628 | learning_rate: 3.386075949367089e-05
Step 628 | epoch: 1.9873417721518987
Step 629 | loss: 0.5840321183204651
Step 629 | grad_norm: 0.9736500978469849
Step 629 | learning_rate: 3.3755274261603375e-05
Step 629 | epoch: 1.990506329113924
Step 630 | loss: 0.49964356422424316
Step 630 | grad_norm: 0.9662789702415466
Step 630 | learning_rate: 3.3649789029535864e-05
Step 630 | epoch: 1.9936708860759493
Step 631 | loss: 0.614854097366333
Step 631 | grad_norm: 1.3486089706420898
Step 631 | learning_rate: 3.354430379746836e-05
Step 631 | epoch: 1.9968354430379747
Step 632 | loss: 0.447020024061203
Step 632 | grad_norm: 0.8513936400413513
Step 632 | learning_rate: 3.343881856540085e-05
Step 632 | epoch: 2.0
Step 633 | loss: 0.36070650815963745
Step 633 | grad_norm: 0.951077938079834
Step 633 | learning_rate: 3.3333333333333335e-05
Step 633 | epoch: 2.0031645569620253
Step 634 | loss: 0.43311506509780884
Step 634 | grad_norm: 0.8267393708229065
Step 634 | learning_rate: 3.322784810126582e-05
Step 634 | epoch: 2.0063291139240507
Step 635 | loss: 0.5217833518981934
Step 635 | grad_norm: 1.1251920461654663
Step 635 | learning_rate: 3.312236286919831e-05
Step 635 | epoch: 2.009493670886076
Step 636 | loss: 0.41494831442832947
Step 636 | grad_norm: 0.9010170698165894
Step 636 | learning_rate: 3.3016877637130806e-05
Step 636 | epoch: 2.0126582278481013
Step 637 | loss: 0.42041683197021484
Step 637 | grad_norm: 0.783287763595581
Step 637 | learning_rate: 3.291139240506329e-05
Step 637 | epoch: 2.0158227848101267
Step 638 | loss: 0.541822075843811
Step 638 | grad_norm: 0.944614589214325
Step 638 | learning_rate: 3.280590717299578e-05
Step 638 | epoch: 2.018987341772152
Step 639 | loss: 0.3916310667991638
Step 639 | grad_norm: 0.9207156896591187
Step 639 | learning_rate: 3.270042194092827e-05
Step 639 | epoch: 2.0221518987341773
Step 640 | loss: 0.4181230068206787
Step 640 | grad_norm: 0.8999670147895813
Step 640 | learning_rate: 3.2594936708860766e-05
Step 640 | epoch: 2.0253164556962027
Step 641 | loss: 0.530968189239502
Step 641 | grad_norm: 1.2532203197479248
Step 641 | learning_rate: 3.248945147679325e-05
Step 641 | epoch: 2.028481012658228
Step 642 | loss: 0.458990216255188
Step 642 | grad_norm: 0.8794564008712769
Step 642 | learning_rate: 3.2383966244725736e-05
Step 642 | epoch: 2.0316455696202533
Step 643 | loss: 0.5937583446502686
Step 643 | grad_norm: 1.0099983215332031
Step 643 | learning_rate: 3.227848101265823e-05
Step 643 | epoch: 2.0348101265822787
Step 644 | loss: 0.44578680396080017
Step 644 | grad_norm: 1.0975568294525146
Step 644 | learning_rate: 3.217299578059072e-05
Step 644 | epoch: 2.037974683544304
Step 645 | loss: 0.49646973609924316
Step 645 | grad_norm: 0.9895682334899902
Step 645 | learning_rate: 3.2067510548523214e-05
Step 645 | epoch: 2.041139240506329
Step 646 | loss: 0.36230164766311646
Step 646 | grad_norm: 0.861612856388092
Step 646 | learning_rate: 3.1962025316455695e-05
Step 646 | epoch: 2.0443037974683542
Step 647 | loss: 0.3875674605369568
Step 647 | grad_norm: 1.119240641593933
Step 647 | learning_rate: 3.185654008438819e-05
Step 647 | epoch: 2.0474683544303796
Step 648 | loss: 0.4525017738342285
Step 648 | grad_norm: 1.1220346689224243
Step 648 | learning_rate: 3.175105485232068e-05
Step 648 | epoch: 2.050632911392405
Step 649 | loss: 0.46182164549827576
Step 649 | grad_norm: 1.0028047561645508
Step 649 | learning_rate: 3.1645569620253167e-05
Step 649 | epoch: 2.0537974683544302
Step 650 | loss: 0.334467351436615
Step 650 | grad_norm: 1.0684889554977417
Step 650 | learning_rate: 3.1540084388185655e-05
Step 650 | epoch: 2.0569620253164556
Step 651 | loss: 0.45676496624946594
Step 651 | grad_norm: 1.1232051849365234
Step 651 | learning_rate: 3.143459915611814e-05
Step 651 | epoch: 2.060126582278481
Step 652 | loss: 0.4406774938106537
Step 652 | grad_norm: 1.2121797800064087
Step 652 | learning_rate: 3.132911392405064e-05
Step 652 | epoch: 2.0632911392405062
Step 653 | loss: 0.48791563510894775
Step 653 | grad_norm: 1.5005377531051636
Step 653 | learning_rate: 3.1223628691983126e-05
Step 653 | epoch: 2.0664556962025316
Step 654 | loss: 0.35978448390960693
Step 654 | grad_norm: 1.11055588722229
Step 654 | learning_rate: 3.111814345991561e-05
Step 654 | epoch: 2.069620253164557
Step 655 | loss: 0.4577191472053528
Step 655 | grad_norm: 1.3644262552261353
Step 655 | learning_rate: 3.10126582278481e-05
Step 655 | epoch: 2.0727848101265822
Step 656 | loss: 0.3683643341064453
Step 656 | grad_norm: 0.9951638579368591
Step 656 | learning_rate: 3.090717299578059e-05
Step 656 | epoch: 2.0759493670886076
Step 657 | loss: 0.4869691729545593
Step 657 | grad_norm: 1.2510442733764648
Step 657 | learning_rate: 3.0801687763713086e-05
Step 657 | epoch: 2.079113924050633
Step 658 | loss: 0.3752232789993286
Step 658 | grad_norm: 1.1648958921432495
Step 658 | learning_rate: 3.0696202531645574e-05
Step 658 | epoch: 2.0822784810126582
Step 659 | loss: 0.39267826080322266
Step 659 | grad_norm: 1.1735267639160156
Step 659 | learning_rate: 3.059071729957806e-05
Step 659 | epoch: 2.0854430379746836
Step 660 | loss: 0.33287903666496277
Step 660 | grad_norm: 1.384972333908081
Step 660 | learning_rate: 3.048523206751055e-05
Step 660 | epoch: 2.088607594936709
Step 661 | loss: 0.4417512118816376
Step 661 | grad_norm: 1.5091999769210815
Step 661 | learning_rate: 3.0379746835443042e-05
Step 661 | epoch: 2.0917721518987342
Step 662 | loss: 0.4192597270011902
Step 662 | grad_norm: 1.271187424659729
Step 662 | learning_rate: 3.027426160337553e-05
Step 662 | epoch: 2.0949367088607596
Step 663 | loss: 0.47685205936431885
Step 663 | grad_norm: 1.5002892017364502
Step 663 | learning_rate: 3.0168776371308015e-05
Step 663 | epoch: 2.098101265822785
Step 664 | loss: 0.36955752968788147
Step 664 | grad_norm: 1.6914117336273193
Step 664 | learning_rate: 3.0063291139240506e-05
Step 664 | epoch: 2.1012658227848102
Step 665 | loss: 0.4011480212211609
Step 665 | grad_norm: 1.1316391229629517
Step 665 | learning_rate: 2.9957805907172998e-05
Step 665 | epoch: 2.1044303797468356
Step 666 | loss: 0.364671915769577
Step 666 | grad_norm: 1.2455161809921265
Step 666 | learning_rate: 2.985232067510549e-05
Step 666 | epoch: 2.107594936708861
Step 667 | loss: 0.39975011348724365
Step 667 | grad_norm: 1.7461668252944946
Step 667 | learning_rate: 2.9746835443037974e-05
Step 667 | epoch: 2.1107594936708862
Step 668 | loss: 0.43942463397979736
Step 668 | grad_norm: 1.4204953908920288
Step 668 | learning_rate: 2.9641350210970466e-05
Step 668 | epoch: 2.1139240506329116
Step 669 | loss: 0.44950249791145325
Step 669 | grad_norm: 1.218053936958313
Step 669 | learning_rate: 2.9535864978902954e-05
Step 669 | epoch: 2.117088607594937
Step 670 | loss: 0.42626768350601196
Step 670 | grad_norm: 1.2430903911590576
Step 670 | learning_rate: 2.9430379746835446e-05
Step 670 | epoch: 2.1202531645569622
Step 671 | loss: 0.5461037755012512
Step 671 | grad_norm: 1.1429405212402344
Step 671 | learning_rate: 2.9324894514767937e-05
Step 671 | epoch: 2.1234177215189876
Step 672 | loss: 0.4795549809932709
Step 672 | grad_norm: 1.1745940446853638
Step 672 | learning_rate: 2.9219409282700422e-05
Step 672 | epoch: 2.1265822784810124
Step 673 | loss: 0.4858074188232422
Step 673 | grad_norm: 1.1719883680343628
Step 673 | learning_rate: 2.9113924050632914e-05
Step 673 | epoch: 2.1297468354430378
Step 674 | loss: 0.46753907203674316
Step 674 | grad_norm: 1.6904999017715454
Step 674 | learning_rate: 2.9008438818565402e-05
Step 674 | epoch: 2.132911392405063
Step 675 | loss: 0.5475621819496155
Step 675 | grad_norm: 1.2381994724273682
Step 675 | learning_rate: 2.8902953586497894e-05
Step 675 | epoch: 2.1360759493670884
Step 676 | loss: 0.5177837014198303
Step 676 | grad_norm: 1.402740716934204
Step 676 | learning_rate: 2.879746835443038e-05
Step 676 | epoch: 2.1392405063291138
Step 677 | loss: 0.5876615047454834
Step 677 | grad_norm: 1.3746185302734375
Step 677 | learning_rate: 2.869198312236287e-05
Step 677 | epoch: 2.142405063291139
Step 678 | loss: 0.3691447079181671
Step 678 | grad_norm: 1.1000248193740845
Step 678 | learning_rate: 2.858649789029536e-05
Step 678 | epoch: 2.1455696202531644
Step 679 | loss: 0.36345863342285156
Step 679 | grad_norm: 1.3735144138336182
Step 679 | learning_rate: 2.848101265822785e-05
Step 679 | epoch: 2.1487341772151898
Step 680 | loss: 0.42857661843299866
Step 680 | grad_norm: 1.368412971496582
Step 680 | learning_rate: 2.8375527426160338e-05
Step 680 | epoch: 2.151898734177215
Step 681 | loss: 0.4439527690410614
Step 681 | grad_norm: 1.0647621154785156
Step 681 | learning_rate: 2.8270042194092826e-05
Step 681 | epoch: 2.1550632911392404
Step 682 | loss: 0.6562942266464233
Step 682 | grad_norm: 1.2341068983078003
Step 682 | learning_rate: 2.8164556962025318e-05
Step 682 | epoch: 2.1582278481012658
Step 683 | loss: 0.42097070813179016
Step 683 | grad_norm: 1.1525121927261353
Step 683 | learning_rate: 2.805907172995781e-05
Step 683 | epoch: 2.161392405063291
Step 684 | loss: 0.5154660940170288
Step 684 | grad_norm: 1.1774624586105347
Step 684 | learning_rate: 2.7953586497890294e-05
Step 684 | epoch: 2.1645569620253164
Step 685 | loss: 0.448095440864563
Step 685 | grad_norm: 1.0511620044708252
Step 685 | learning_rate: 2.7848101265822786e-05
Step 685 | epoch: 2.1677215189873418
Step 686 | loss: 0.3542836606502533
Step 686 | grad_norm: 1.3425053358078003
Step 686 | learning_rate: 2.7742616033755274e-05
Step 686 | epoch: 2.170886075949367
Step 687 | loss: 0.5107202529907227
Step 687 | grad_norm: 1.1508533954620361
Step 687 | learning_rate: 2.7637130801687766e-05
Step 687 | epoch: 2.1740506329113924
Step 688 | loss: 0.5259804725646973
Step 688 | grad_norm: 1.1976306438446045
Step 688 | learning_rate: 2.7531645569620257e-05
Step 688 | epoch: 2.1772151898734178
Step 689 | loss: 0.3642163872718811
Step 689 | grad_norm: 1.3848936557769775
Step 689 | learning_rate: 2.7426160337552742e-05
Step 689 | epoch: 2.180379746835443
Step 690 | loss: 0.5365241765975952
Step 690 | grad_norm: 1.3450556993484497
Step 690 | learning_rate: 2.7320675105485234e-05
Step 690 | epoch: 2.1835443037974684
Step 691 | loss: 0.2762356102466583
Step 691 | grad_norm: 0.8153225183486938
Step 691 | learning_rate: 2.7215189873417722e-05
Step 691 | epoch: 2.1867088607594938
Step 692 | loss: 0.3631312847137451
Step 692 | grad_norm: 1.0515878200531006
Step 692 | learning_rate: 2.7109704641350213e-05
Step 692 | epoch: 2.189873417721519
Step 693 | loss: 0.4952828586101532
Step 693 | grad_norm: 1.182857632637024
Step 693 | learning_rate: 2.7004219409282698e-05
Step 693 | epoch: 2.1930379746835444
Step 694 | loss: 0.4383067786693573
Step 694 | grad_norm: 1.2227702140808105
Step 694 | learning_rate: 2.689873417721519e-05
Step 694 | epoch: 2.1962025316455698
Step 695 | loss: 0.4671188294887543
Step 695 | grad_norm: 1.228356122970581
Step 695 | learning_rate: 2.679324894514768e-05
Step 695 | epoch: 2.199367088607595
Step 696 | loss: 0.34945330023765564
Step 696 | grad_norm: 0.9791297912597656
Step 696 | learning_rate: 2.6687763713080173e-05
Step 696 | epoch: 2.2025316455696204
Step 697 | loss: 0.40715038776397705
Step 697 | grad_norm: 1.11680269241333
Step 697 | learning_rate: 2.6582278481012658e-05
Step 697 | epoch: 2.2056962025316458
Step 698 | loss: 0.3733868896961212
Step 698 | grad_norm: 1.7364230155944824
Step 698 | learning_rate: 2.6476793248945146e-05
Step 698 | epoch: 2.208860759493671
Step 699 | loss: 0.3906623423099518
Step 699 | grad_norm: 1.2972737550735474
Step 699 | learning_rate: 2.6371308016877638e-05
Step 699 | epoch: 2.212025316455696
Step 700 | loss: 0.3573915958404541
Step 700 | grad_norm: 1.3785490989685059
Step 700 | learning_rate: 2.626582278481013e-05
Step 700 | epoch: 2.2151898734177213
Step 701 | loss: 0.4743504524230957
Step 701 | grad_norm: 1.629886507987976
Step 701 | learning_rate: 2.616033755274262e-05
Step 701 | epoch: 2.2183544303797467
Step 702 | loss: 0.4681328237056732
Step 702 | grad_norm: 1.3417551517486572
Step 702 | learning_rate: 2.6054852320675106e-05
Step 702 | epoch: 2.221518987341772
Step 703 | loss: 0.6067330241203308
Step 703 | grad_norm: 1.5317094326019287
Step 703 | learning_rate: 2.5949367088607597e-05
Step 703 | epoch: 2.2246835443037973
Step 704 | loss: 0.43700313568115234
Step 704 | grad_norm: 1.1236505508422852
Step 704 | learning_rate: 2.5843881856540085e-05
Step 704 | epoch: 2.2278481012658227
Step 705 | loss: 0.3167029023170471
Step 705 | grad_norm: 0.8830735087394714
Step 705 | learning_rate: 2.5738396624472577e-05
Step 705 | epoch: 2.231012658227848
Step 706 | loss: 0.5042577981948853
Step 706 | grad_norm: 1.2760432958602905
Step 706 | learning_rate: 2.5632911392405062e-05
Step 706 | epoch: 2.2341772151898733
Step 707 | loss: 0.451257586479187
Step 707 | grad_norm: 1.2232946157455444
Step 707 | learning_rate: 2.5527426160337553e-05
Step 707 | epoch: 2.2373417721518987
Step 708 | loss: 0.5487867593765259
Step 708 | grad_norm: 1.5575497150421143
Step 708 | learning_rate: 2.5421940928270045e-05
Step 708 | epoch: 2.240506329113924
Step 709 | loss: 0.48513686656951904
Step 709 | grad_norm: 1.197838306427002
Step 709 | learning_rate: 2.5316455696202533e-05
Step 709 | epoch: 2.2436708860759493
Step 710 | loss: 0.49955451488494873
Step 710 | grad_norm: 1.2563788890838623
Step 710 | learning_rate: 2.521097046413502e-05
Step 710 | epoch: 2.2468354430379747
Step 711 | loss: 0.5541561245918274
Step 711 | grad_norm: 1.3787078857421875
Step 711 | learning_rate: 2.510548523206751e-05
Step 711 | epoch: 2.25
Step 712 | loss: 0.42764660716056824
Step 712 | grad_norm: 1.38297700881958
Step 712 | learning_rate: 2.5e-05
Step 712 | epoch: 2.2531645569620253
Step 713 | loss: 0.4911225438117981
Step 713 | grad_norm: 1.3959670066833496
Step 713 | learning_rate: 2.4894514767932493e-05
Step 713 | epoch: 2.2563291139240507
Step 714 | loss: 0.412783145904541
Step 714 | grad_norm: 1.2349045276641846
Step 714 | learning_rate: 2.478902953586498e-05
Step 714 | epoch: 2.259493670886076
Step 715 | loss: 0.4125216007232666
Step 715 | grad_norm: 1.4254403114318848
Step 715 | learning_rate: 2.468354430379747e-05
Step 715 | epoch: 2.2626582278481013
Step 716 | loss: 0.46014097332954407
Step 716 | grad_norm: 1.3920389413833618
Step 716 | learning_rate: 2.4578059071729957e-05
Step 716 | epoch: 2.2658227848101267
Step 717 | loss: 0.3535889983177185
Step 717 | grad_norm: 1.335766315460205
Step 717 | learning_rate: 2.447257383966245e-05
Step 717 | epoch: 2.268987341772152
Step 718 | loss: 0.3972194790840149
Step 718 | grad_norm: 1.2922797203063965
Step 718 | learning_rate: 2.4367088607594937e-05
Step 718 | epoch: 2.2721518987341773
Step 719 | loss: 0.5366496443748474
Step 719 | grad_norm: 1.5031733512878418
Step 719 | learning_rate: 2.426160337552743e-05
Step 719 | epoch: 2.2753164556962027
Step 720 | loss: 0.38510602712631226
Step 720 | grad_norm: 1.0576750040054321
Step 720 | learning_rate: 2.4156118143459917e-05
Step 720 | epoch: 2.278481012658228
Step 721 | loss: 0.391649454832077
Step 721 | grad_norm: 1.241023302078247
Step 721 | learning_rate: 2.4050632911392405e-05
Step 721 | epoch: 2.2816455696202533
Step 722 | loss: 0.4156237840652466
Step 722 | grad_norm: 1.4111487865447998
Step 722 | learning_rate: 2.3945147679324893e-05
Step 722 | epoch: 2.2848101265822787
Step 723 | loss: 0.4939190149307251
Step 723 | grad_norm: 1.3827935457229614
Step 723 | learning_rate: 2.3839662447257385e-05
Step 723 | epoch: 2.287974683544304
Step 724 | loss: 0.3860371708869934
Step 724 | grad_norm: 1.0576990842819214
Step 724 | learning_rate: 2.3734177215189873e-05
Step 724 | epoch: 2.291139240506329
Step 725 | loss: 0.44937798380851746
Step 725 | grad_norm: 1.148653268814087
Step 725 | learning_rate: 2.3628691983122365e-05
Step 725 | epoch: 2.2943037974683547
Step 726 | loss: 0.32107919454574585
Step 726 | grad_norm: 1.133927345275879
Step 726 | learning_rate: 2.3523206751054856e-05
Step 726 | epoch: 2.2974683544303796
Step 727 | loss: 0.43044352531433105
Step 727 | grad_norm: 1.1251916885375977
Step 727 | learning_rate: 2.341772151898734e-05
Step 727 | epoch: 2.300632911392405
Step 728 | loss: 0.33442047238349915
Step 728 | grad_norm: 1.5018385648727417
Step 728 | learning_rate: 2.3312236286919833e-05
Step 728 | epoch: 2.3037974683544302
Step 729 | loss: 0.6007564663887024
Step 729 | grad_norm: 1.5107557773590088
Step 729 | learning_rate: 2.320675105485232e-05
Step 729 | epoch: 2.3069620253164556
Step 730 | loss: 0.4919874370098114
Step 730 | grad_norm: 1.2476218938827515
Step 730 | learning_rate: 2.3101265822784813e-05
Step 730 | epoch: 2.310126582278481
Step 731 | loss: 0.4253961443901062
Step 731 | grad_norm: 1.2354692220687866
Step 731 | learning_rate: 2.29957805907173e-05
Step 731 | epoch: 2.3132911392405062
Step 732 | loss: 0.3974885642528534
Step 732 | grad_norm: 1.2357876300811768
Step 732 | learning_rate: 2.2890295358649792e-05
Step 732 | epoch: 2.3164556962025316
Step 733 | loss: 0.42465925216674805
Step 733 | grad_norm: 1.0945543050765991
Step 733 | learning_rate: 2.278481012658228e-05
Step 733 | epoch: 2.319620253164557
Step 734 | loss: 0.4247453510761261
Step 734 | grad_norm: 0.9813193678855896
Step 734 | learning_rate: 2.267932489451477e-05
Step 734 | epoch: 2.3227848101265822
Step 735 | loss: 0.45230764150619507
Step 735 | grad_norm: 1.1987032890319824
Step 735 | learning_rate: 2.2573839662447257e-05
Step 735 | epoch: 2.3259493670886076
Step 736 | loss: 0.37262240052223206
Step 736 | grad_norm: 1.056789755821228
Step 736 | learning_rate: 2.246835443037975e-05
Step 736 | epoch: 2.329113924050633
Step 737 | loss: 0.451377809047699
Step 737 | grad_norm: 1.3844536542892456
Step 737 | learning_rate: 2.2362869198312237e-05
Step 737 | epoch: 2.3322784810126582
Step 738 | loss: 0.37674644589424133
Step 738 | grad_norm: 1.5244486331939697
Step 738 | learning_rate: 2.225738396624473e-05
Step 738 | epoch: 2.3354430379746836
Step 739 | loss: 0.384476900100708
Step 739 | grad_norm: 1.2618107795715332
Step 739 | learning_rate: 2.2151898734177217e-05
Step 739 | epoch: 2.338607594936709
Step 740 | loss: 0.45805713534355164
Step 740 | grad_norm: 1.3409944772720337
Step 740 | learning_rate: 2.2046413502109705e-05
Step 740 | epoch: 2.3417721518987342
Step 741 | loss: 0.40236756205558777
Step 741 | grad_norm: 1.1991316080093384
Step 741 | learning_rate: 2.1940928270042196e-05
Step 741 | epoch: 2.3449367088607596
Step 742 | loss: 0.43146032094955444
Step 742 | grad_norm: 1.2498506307601929
Step 742 | learning_rate: 2.1835443037974685e-05
Step 742 | epoch: 2.348101265822785
Step 743 | loss: 0.41162872314453125
Step 743 | grad_norm: 1.1369980573654175
Step 743 | learning_rate: 2.1729957805907176e-05
Step 743 | epoch: 2.3512658227848102
Step 744 | loss: 0.47111693024635315
Step 744 | grad_norm: 1.1375107765197754
Step 744 | learning_rate: 2.1624472573839664e-05
Step 744 | epoch: 2.3544303797468356
Step 745 | loss: 0.46608150005340576
Step 745 | grad_norm: 1.3303166627883911
Step 745 | learning_rate: 2.1518987341772153e-05
Step 745 | epoch: 2.357594936708861
Step 746 | loss: 0.4925805926322937
Step 746 | grad_norm: 1.3196889162063599
Step 746 | learning_rate: 2.141350210970464e-05
Step 746 | epoch: 2.3607594936708862
Step 747 | loss: 0.3792881965637207
Step 747 | grad_norm: 1.1926196813583374
Step 747 | learning_rate: 2.1308016877637132e-05
Step 747 | epoch: 2.3639240506329116
Step 748 | loss: 0.4237399697303772
Step 748 | grad_norm: 1.3282196521759033
Step 748 | learning_rate: 2.120253164556962e-05
Step 748 | epoch: 2.367088607594937
Step 749 | loss: 0.29313164949417114
Step 749 | grad_norm: 1.0114938020706177
Step 749 | learning_rate: 2.1097046413502112e-05
Step 749 | epoch: 2.370253164556962
Step 750 | loss: 0.3137999176979065
Step 750 | grad_norm: 1.3534729480743408
Step 750 | learning_rate: 2.09915611814346e-05
Step 750 | epoch: 2.3734177215189876
Step 751 | loss: 0.4426114559173584
Step 751 | grad_norm: 1.3736562728881836
Step 751 | learning_rate: 2.088607594936709e-05
Step 751 | epoch: 2.3765822784810124
Step 752 | loss: 0.6026413440704346
Step 752 | grad_norm: 1.6181938648223877
Step 752 | learning_rate: 2.0780590717299577e-05
Step 752 | epoch: 2.379746835443038
Step 753 | loss: 0.5015957355499268
Step 753 | grad_norm: 1.3231464624404907
Step 753 | learning_rate: 2.067510548523207e-05
Step 753 | epoch: 2.382911392405063
Step 754 | loss: 0.40327388048171997
Step 754 | grad_norm: 1.2795647382736206
Step 754 | learning_rate: 2.056962025316456e-05
Step 754 | epoch: 2.3860759493670884
Step 755 | loss: 0.48027732968330383
Step 755 | grad_norm: 1.8777285814285278
Step 755 | learning_rate: 2.0464135021097048e-05
Step 755 | epoch: 2.3892405063291138
Step 756 | loss: 0.45137491822242737
Step 756 | grad_norm: 1.4353963136672974
Step 756 | learning_rate: 2.0358649789029536e-05
Step 756 | epoch: 2.392405063291139
Step 757 | loss: 0.34854769706726074
Step 757 | grad_norm: 0.9958810806274414
Step 757 | learning_rate: 2.0253164556962025e-05
Step 757 | epoch: 2.3955696202531644
Step 758 | loss: 0.4855780005455017
Step 758 | grad_norm: 1.5666266679763794
Step 758 | learning_rate: 2.0147679324894516e-05
Step 758 | epoch: 2.3987341772151898
Step 759 | loss: 0.40075212717056274
Step 759 | grad_norm: 1.2672460079193115
Step 759 | learning_rate: 2.0042194092827004e-05
Step 759 | epoch: 2.401898734177215
Step 760 | loss: 0.3745720386505127
Step 760 | grad_norm: 1.1857287883758545
Step 760 | learning_rate: 1.9936708860759496e-05
Step 760 | epoch: 2.4050632911392404
Step 761 | loss: 0.47039684653282166
Step 761 | grad_norm: 1.2683864831924438
Step 761 | learning_rate: 1.9831223628691984e-05
Step 761 | epoch: 2.4082278481012658
Step 762 | loss: 0.36162352561950684
Step 762 | grad_norm: 1.406448245048523
Step 762 | learning_rate: 1.9725738396624476e-05
Step 762 | epoch: 2.411392405063291
Step 763 | loss: 0.4344692826271057
Step 763 | grad_norm: 1.1927835941314697
Step 763 | learning_rate: 1.962025316455696e-05
Step 763 | epoch: 2.4145569620253164
Step 764 | loss: 0.47455498576164246
Step 764 | grad_norm: 1.152687430381775
Step 764 | learning_rate: 1.9514767932489452e-05
Step 764 | epoch: 2.4177215189873418
Step 765 | loss: 0.3509383201599121
Step 765 | grad_norm: 1.254997730255127
Step 765 | learning_rate: 1.940928270042194e-05
Step 765 | epoch: 2.420886075949367
Step 766 | loss: 0.6094145178794861
Step 766 | grad_norm: 1.3064172267913818
Step 766 | learning_rate: 1.9303797468354432e-05
Step 766 | epoch: 2.4240506329113924
Step 767 | loss: 0.3820539712905884
Step 767 | grad_norm: 1.3690602779388428
Step 767 | learning_rate: 1.919831223628692e-05
Step 767 | epoch: 2.4272151898734178
Step 768 | loss: 0.4909709692001343
Step 768 | grad_norm: 1.952404499053955
Step 768 | learning_rate: 1.9092827004219412e-05
Step 768 | epoch: 2.430379746835443
Step 769 | loss: 0.42049118876457214
Step 769 | grad_norm: 1.3924164772033691
Step 769 | learning_rate: 1.89873417721519e-05
Step 769 | epoch: 2.4335443037974684
Step 770 | loss: 0.4156940281391144
Step 770 | grad_norm: 1.376695990562439
Step 770 | learning_rate: 1.8881856540084388e-05
Step 770 | epoch: 2.4367088607594938
Step 771 | loss: 0.33756881952285767
Step 771 | grad_norm: 1.091084361076355
Step 771 | learning_rate: 1.877637130801688e-05
Step 771 | epoch: 2.439873417721519
Step 772 | loss: 0.38338568806648254
Step 772 | grad_norm: 1.396305799484253
Step 772 | learning_rate: 1.8670886075949368e-05
Step 772 | epoch: 2.4430379746835444
Step 773 | loss: 0.3914216160774231
Step 773 | grad_norm: 1.8840234279632568
Step 773 | learning_rate: 1.856540084388186e-05
Step 773 | epoch: 2.4462025316455698
Step 774 | loss: 0.48301345109939575
Step 774 | grad_norm: 1.838049054145813
Step 774 | learning_rate: 1.8459915611814348e-05
Step 774 | epoch: 2.449367088607595
Step 775 | loss: 0.3972676396369934
Step 775 | grad_norm: 1.4099972248077393
Step 775 | learning_rate: 1.8354430379746836e-05
Step 775 | epoch: 2.4525316455696204
Step 776 | loss: 0.48267215490341187
Step 776 | grad_norm: 1.154249668121338
Step 776 | learning_rate: 1.8248945147679324e-05
Step 776 | epoch: 2.4556962025316453
Step 777 | loss: 0.5738095641136169
Step 777 | grad_norm: 1.3879814147949219
Step 777 | learning_rate: 1.8143459915611816e-05
Step 777 | epoch: 2.458860759493671
Step 778 | loss: 0.40024659037590027
Step 778 | grad_norm: 1.729736089706421
Step 778 | learning_rate: 1.8037974683544304e-05
Step 778 | epoch: 2.462025316455696
Step 779 | loss: 0.5673269629478455
Step 779 | grad_norm: 1.5687894821166992
Step 779 | learning_rate: 1.7932489451476795e-05
Step 779 | epoch: 2.4651898734177213
Step 780 | loss: 0.40054795145988464
Step 780 | grad_norm: 1.2771879434585571
Step 780 | learning_rate: 1.7827004219409284e-05
Step 780 | epoch: 2.4683544303797467
Step 781 | loss: 0.44557085633277893
Step 781 | grad_norm: 1.3299363851547241
Step 781 | learning_rate: 1.7721518987341772e-05
Step 781 | epoch: 2.471518987341772
Step 782 | loss: 0.472432404756546
Step 782 | grad_norm: 1.2316888570785522
Step 782 | learning_rate: 1.7616033755274263e-05
Step 782 | epoch: 2.4746835443037973
Step 783 | loss: 0.4922972619533539
Step 783 | grad_norm: 1.4919450283050537
Step 783 | learning_rate: 1.751054852320675e-05
Step 783 | epoch: 2.4778481012658227
Step 784 | loss: 0.43519648909568787
Step 784 | grad_norm: 1.29120934009552
Step 784 | learning_rate: 1.7405063291139243e-05
Step 784 | epoch: 2.481012658227848
Step 785 | loss: 0.4377647340297699
Step 785 | grad_norm: 1.1894481182098389
Step 785 | learning_rate: 1.729957805907173e-05
Step 785 | epoch: 2.4841772151898733
Step 786 | loss: 0.38558709621429443
Step 786 | grad_norm: 1.2918721437454224
Step 786 | learning_rate: 1.719409282700422e-05
Step 786 | epoch: 2.4873417721518987
Step 787 | loss: 0.5033963918685913
Step 787 | grad_norm: 1.4808025360107422
Step 787 | learning_rate: 1.7088607594936708e-05
Step 787 | epoch: 2.490506329113924
Step 788 | loss: 0.35359203815460205
Step 788 | grad_norm: 1.3832217454910278
Step 788 | learning_rate: 1.69831223628692e-05
Step 788 | epoch: 2.4936708860759493
Step 789 | loss: 0.35301917791366577
Step 789 | grad_norm: 1.209168791770935
Step 789 | learning_rate: 1.6877637130801688e-05
Step 789 | epoch: 2.4968354430379747
Step 790 | loss: 0.4404754042625427
Step 790 | grad_norm: 1.3015294075012207
Step 790 | learning_rate: 1.677215189873418e-05
Step 790 | epoch: 2.5
Step 791 | loss: 0.498582124710083
Step 791 | grad_norm: 1.4119735956192017
Step 791 | learning_rate: 1.6666666666666667e-05
Step 791 | epoch: 2.5031645569620253
Step 792 | loss: 0.5478314757347107
Step 792 | grad_norm: 1.351637601852417
Step 792 | learning_rate: 1.6561181434599156e-05
Step 792 | epoch: 2.5063291139240507
Step 793 | loss: 0.5514391660690308
Step 793 | grad_norm: 1.3190648555755615
Step 793 | learning_rate: 1.6455696202531644e-05
Step 793 | epoch: 2.509493670886076
Step 794 | loss: 0.5301554799079895
Step 794 | grad_norm: 1.8451558351516724
Step 794 | learning_rate: 1.6350210970464135e-05
Step 794 | epoch: 2.5126582278481013
Step 795 | loss: 0.4634973108768463
Step 795 | grad_norm: 1.4220842123031616
Step 795 | learning_rate: 1.6244725738396624e-05
Step 795 | epoch: 2.5158227848101267
Step 796 | loss: 0.38874515891075134
Step 796 | grad_norm: 1.0715540647506714
Step 796 | learning_rate: 1.6139240506329115e-05
Step 796 | epoch: 2.518987341772152
Step 797 | loss: 0.5239011645317078
Step 797 | grad_norm: 1.6158915758132935
Step 797 | learning_rate: 1.6033755274261607e-05
Step 797 | epoch: 2.5221518987341773
Step 798 | loss: 0.41716641187667847
Step 798 | grad_norm: 1.3322136402130127
Step 798 | learning_rate: 1.5928270042194095e-05
Step 798 | epoch: 2.5253164556962027
Step 799 | loss: 0.43354934453964233
Step 799 | grad_norm: 1.2772324085235596
Step 799 | learning_rate: 1.5822784810126583e-05
Step 799 | epoch: 2.528481012658228
Step 800 | loss: 0.5496906638145447
Step 800 | grad_norm: 1.3906630277633667
Step 800 | learning_rate: 1.571729957805907e-05
Step 800 | epoch: 2.5316455696202533
Step 801 | loss: 0.5083223581314087
Step 801 | grad_norm: 1.2687896490097046
Step 801 | learning_rate: 1.5611814345991563e-05
Step 801 | epoch: 2.5348101265822782
Step 802 | loss: 0.44840115308761597
Step 802 | grad_norm: 1.5511960983276367
Step 802 | learning_rate: 1.550632911392405e-05
Step 802 | epoch: 2.537974683544304
Step 803 | loss: 0.29911163449287415
Step 803 | grad_norm: 1.1474086046218872
Step 803 | learning_rate: 1.5400843881856543e-05
Step 803 | epoch: 2.541139240506329
Step 804 | loss: 0.39168256521224976
Step 804 | grad_norm: 1.2287148237228394
Step 804 | learning_rate: 1.529535864978903e-05
Step 804 | epoch: 2.5443037974683547
Step 805 | loss: 0.33324652910232544
Step 805 | grad_norm: 1.0238996744155884
Step 805 | learning_rate: 1.5189873417721521e-05
Step 805 | epoch: 2.5474683544303796
Step 806 | loss: 0.38280025124549866
Step 806 | grad_norm: 1.2054455280303955
Step 806 | learning_rate: 1.5084388185654007e-05
Step 806 | epoch: 2.5506329113924053
Step 807 | loss: 0.5563109517097473
Step 807 | grad_norm: 1.312742829322815
Step 807 | learning_rate: 1.4978902953586499e-05
Step 807 | epoch: 2.5537974683544302
Step 808 | loss: 0.42507055401802063
Step 808 | grad_norm: 1.720819354057312
Step 808 | learning_rate: 1.4873417721518987e-05
Step 808 | epoch: 2.5569620253164556
Step 809 | loss: 0.39016443490982056
Step 809 | grad_norm: 1.6216026544570923
Step 809 | learning_rate: 1.4767932489451477e-05
Step 809 | epoch: 2.560126582278481
Step 810 | loss: 0.3366204798221588
Step 810 | grad_norm: 1.3319116830825806
Step 810 | learning_rate: 1.4662447257383969e-05
Step 810 | epoch: 2.5632911392405062
Step 811 | loss: 0.43773913383483887
Step 811 | grad_norm: 1.4722718000411987
Step 811 | learning_rate: 1.4556962025316457e-05
Step 811 | epoch: 2.5664556962025316
Step 812 | loss: 0.44946691393852234
Step 812 | grad_norm: 1.5018788576126099
Step 812 | learning_rate: 1.4451476793248947e-05
Step 812 | epoch: 2.569620253164557
Step 813 | loss: 0.417479932308197
Step 813 | grad_norm: 1.22462797164917
Step 813 | learning_rate: 1.4345991561181435e-05
Step 813 | epoch: 2.5727848101265822
Step 814 | loss: 0.46823060512542725
Step 814 | grad_norm: 1.3914464712142944
Step 814 | learning_rate: 1.4240506329113925e-05
Step 814 | epoch: 2.5759493670886076
Step 815 | loss: 0.3721095621585846
Step 815 | grad_norm: 1.2619236707687378
Step 815 | learning_rate: 1.4135021097046413e-05
Step 815 | epoch: 2.579113924050633
Step 816 | loss: 0.4352523982524872
Step 816 | grad_norm: 1.3296889066696167
Step 816 | learning_rate: 1.4029535864978905e-05
Step 816 | epoch: 2.5822784810126582
Step 817 | loss: 0.3855554461479187
Step 817 | grad_norm: 1.116819977760315
Step 817 | learning_rate: 1.3924050632911393e-05
Step 817 | epoch: 2.5854430379746836
Step 818 | loss: 0.2548457384109497
Step 818 | grad_norm: 0.9912087321281433
Step 818 | learning_rate: 1.3818565400843883e-05
Step 818 | epoch: 2.588607594936709
Step 819 | loss: 0.3623111844062805
Step 819 | grad_norm: 1.75003981590271
Step 819 | learning_rate: 1.3713080168776371e-05
Step 819 | epoch: 2.5917721518987342
Step 820 | loss: 0.38848429918289185
Step 820 | grad_norm: 1.4184415340423584
Step 820 | learning_rate: 1.3607594936708861e-05
Step 820 | epoch: 2.5949367088607596
Step 821 | loss: 0.33335232734680176
Step 821 | grad_norm: 1.1753901243209839
Step 821 | learning_rate: 1.3502109704641349e-05
Step 821 | epoch: 2.598101265822785
Step 822 | loss: 0.272773802280426
Step 822 | grad_norm: 1.0833895206451416
Step 822 | learning_rate: 1.339662447257384e-05
Step 822 | epoch: 2.6012658227848102
Step 823 | loss: 0.5173813700675964
Step 823 | grad_norm: 1.423484444618225
Step 823 | learning_rate: 1.3291139240506329e-05
Step 823 | epoch: 2.6044303797468356
Step 824 | loss: 0.348224937915802
Step 824 | grad_norm: 1.1022160053253174
Step 824 | learning_rate: 1.3185654008438819e-05
Step 824 | epoch: 2.607594936708861
Step 825 | loss: 0.42583757638931274
Step 825 | grad_norm: 1.2822972536087036
Step 825 | learning_rate: 1.308016877637131e-05
Step 825 | epoch: 2.6107594936708862
Step 826 | loss: 0.39681315422058105
Step 826 | grad_norm: 1.5936393737792969
Step 826 | learning_rate: 1.2974683544303799e-05
Step 826 | epoch: 2.6139240506329116
Step 827 | loss: 0.5677003860473633
Step 827 | grad_norm: 1.511307716369629
Step 827 | learning_rate: 1.2869198312236289e-05
Step 827 | epoch: 2.617088607594937
Step 828 | loss: 0.36269184947013855
Step 828 | grad_norm: 1.677748203277588
Step 828 | learning_rate: 1.2763713080168777e-05
Step 828 | epoch: 2.620253164556962
Step 829 | loss: 0.4843159317970276
Step 829 | grad_norm: 1.3587048053741455
Step 829 | learning_rate: 1.2658227848101267e-05
Step 829 | epoch: 2.6234177215189876
Step 830 | loss: 0.5317513346672058
Step 830 | grad_norm: 1.3768270015716553
Step 830 | learning_rate: 1.2552742616033755e-05
Step 830 | epoch: 2.6265822784810124
Step 831 | loss: 0.45719146728515625
Step 831 | grad_norm: 1.3096890449523926
Step 831 | learning_rate: 1.2447257383966246e-05
Step 831 | epoch: 2.629746835443038
Step 832 | loss: 0.4394242763519287
Step 832 | grad_norm: 1.576818823814392
Step 832 | learning_rate: 1.2341772151898735e-05
Step 832 | epoch: 2.632911392405063
Step 833 | loss: 0.3765721917152405
Step 833 | grad_norm: 1.1928101778030396
Step 833 | learning_rate: 1.2236286919831224e-05
Step 833 | epoch: 2.6360759493670884
Step 834 | loss: 0.44305726885795593
Step 834 | grad_norm: 1.4485087394714355
Step 834 | learning_rate: 1.2130801687763714e-05
Step 834 | epoch: 2.6392405063291138
Step 835 | loss: 0.3946095108985901
Step 835 | grad_norm: 1.2977911233901978
Step 835 | learning_rate: 1.2025316455696203e-05
Step 835 | epoch: 2.642405063291139
Step 836 | loss: 0.487853467464447
Step 836 | grad_norm: 1.5577858686447144
Step 836 | learning_rate: 1.1919831223628692e-05
Step 836 | epoch: 2.6455696202531644
Step 837 | loss: 0.41598570346832275
Step 837 | grad_norm: 1.5608118772506714
Step 837 | learning_rate: 1.1814345991561182e-05
Step 837 | epoch: 2.6487341772151898
Step 838 | loss: 0.4022181034088135
Step 838 | grad_norm: 1.2799338102340698
Step 838 | learning_rate: 1.170886075949367e-05
Step 838 | epoch: 2.651898734177215
Step 839 | loss: 0.44613417983055115
Step 839 | grad_norm: 1.2802687883377075
Step 839 | learning_rate: 1.160337552742616e-05
Step 839 | epoch: 2.6550632911392404
Step 840 | loss: 0.5089496374130249
Step 840 | grad_norm: 1.9099396467208862
Step 840 | learning_rate: 1.149789029535865e-05
Step 840 | epoch: 2.6582278481012658
Step 841 | loss: 0.49865201115608215
Step 841 | grad_norm: 1.399085521697998
Step 841 | learning_rate: 1.139240506329114e-05
Step 841 | epoch: 2.661392405063291
Step 842 | loss: 0.4451095461845398
Step 842 | grad_norm: 1.2568237781524658
Step 842 | learning_rate: 1.1286919831223628e-05
Step 842 | epoch: 2.6645569620253164
Step 843 | loss: 0.4415501356124878
Step 843 | grad_norm: 1.2142971754074097
Step 843 | learning_rate: 1.1181434599156118e-05
Step 843 | epoch: 2.6677215189873418
Step 844 | loss: 0.40586137771606445
Step 844 | grad_norm: 1.2144728899002075
Step 844 | learning_rate: 1.1075949367088608e-05
Step 844 | epoch: 2.670886075949367
Step 845 | loss: 0.4326307773590088
Step 845 | grad_norm: 1.3954901695251465
Step 845 | learning_rate: 1.0970464135021098e-05
Step 845 | epoch: 2.6740506329113924
Step 846 | loss: 0.4566567540168762
Step 846 | grad_norm: 1.4289048910140991
Step 846 | learning_rate: 1.0864978902953588e-05
Step 846 | epoch: 2.6772151898734178
Step 847 | loss: 0.32460325956344604
Step 847 | grad_norm: 1.0469212532043457
Step 847 | learning_rate: 1.0759493670886076e-05
Step 847 | epoch: 2.680379746835443
Step 848 | loss: 0.5879145860671997
Step 848 | grad_norm: 1.5406522750854492
Step 848 | learning_rate: 1.0654008438818566e-05
Step 848 | epoch: 2.6835443037974684
Step 849 | loss: 0.6050874590873718
Step 849 | grad_norm: 1.6088566780090332
Step 849 | learning_rate: 1.0548523206751056e-05
Step 849 | epoch: 2.6867088607594938
Step 850 | loss: 0.5937021970748901
Step 850 | grad_norm: 1.6938731670379639
Step 850 | learning_rate: 1.0443037974683544e-05
Step 850 | epoch: 2.689873417721519
Step 851 | loss: 0.3844429850578308
Step 851 | grad_norm: 1.2460031509399414
Step 851 | learning_rate: 1.0337552742616034e-05
Step 851 | epoch: 2.6930379746835444
Step 852 | loss: 0.44643697142601013
Step 852 | grad_norm: 1.4520317316055298
Step 852 | learning_rate: 1.0232067510548524e-05
Step 852 | epoch: 2.6962025316455698
Step 853 | loss: 0.430442750453949
Step 853 | grad_norm: 1.3083364963531494
Step 853 | learning_rate: 1.0126582278481012e-05
Step 853 | epoch: 2.6993670886075947
Step 854 | loss: 0.4670162498950958
Step 854 | grad_norm: 1.188180923461914
Step 854 | learning_rate: 1.0021097046413502e-05
Step 854 | epoch: 2.7025316455696204
Step 855 | loss: 0.37348562479019165
Step 855 | grad_norm: 1.1975034475326538
Step 855 | learning_rate: 9.915611814345992e-06
Step 855 | epoch: 2.7056962025316453
Step 856 | loss: 0.4474088251590729
Step 856 | grad_norm: 1.6605277061462402
Step 856 | learning_rate: 9.81012658227848e-06
Step 856 | epoch: 2.708860759493671
Step 857 | loss: 0.5586187839508057
Step 857 | grad_norm: 1.4606388807296753
Step 857 | learning_rate: 9.70464135021097e-06
Step 857 | epoch: 2.712025316455696
Step 858 | loss: 0.39960241317749023
Step 858 | grad_norm: 1.3906834125518799
Step 858 | learning_rate: 9.59915611814346e-06
Step 858 | epoch: 2.7151898734177218
Step 859 | loss: 0.4305083155632019
Step 859 | grad_norm: 1.4064912796020508
Step 859 | learning_rate: 9.49367088607595e-06
Step 859 | epoch: 2.7183544303797467
Step 860 | loss: 0.5630558729171753
Step 860 | grad_norm: 1.8635895252227783
Step 860 | learning_rate: 9.38818565400844e-06
Step 860 | epoch: 2.721518987341772
Step 861 | loss: 0.4429367184638977
Step 861 | grad_norm: 1.2658030986785889
Step 861 | learning_rate: 9.28270042194093e-06
Step 861 | epoch: 2.7246835443037973
Step 862 | loss: 0.34920555353164673
Step 862 | grad_norm: 1.7052648067474365
Step 862 | learning_rate: 9.177215189873418e-06
Step 862 | epoch: 2.7278481012658227
Step 863 | loss: 0.4376428425312042
Step 863 | grad_norm: 1.2199677228927612
Step 863 | learning_rate: 9.071729957805908e-06
Step 863 | epoch: 2.731012658227848
Step 864 | loss: 0.36028605699539185
Step 864 | grad_norm: 1.1625922918319702
Step 864 | learning_rate: 8.966244725738398e-06
Step 864 | epoch: 2.7341772151898733
Step 865 | loss: 0.46684321761131287
Step 865 | grad_norm: 1.6612884998321533
Step 865 | learning_rate: 8.860759493670886e-06
Step 865 | epoch: 2.7373417721518987
Step 866 | loss: 0.4772607088088989
Step 866 | grad_norm: 1.1758384704589844
Step 866 | learning_rate: 8.755274261603376e-06
Step 866 | epoch: 2.740506329113924
Step 867 | loss: 0.4296415150165558
Step 867 | grad_norm: 1.3490397930145264
Step 867 | learning_rate: 8.649789029535866e-06
Step 867 | epoch: 2.7436708860759493
Step 868 | loss: 0.4052780270576477
Step 868 | grad_norm: 1.1712265014648438
Step 868 | learning_rate: 8.544303797468354e-06
Step 868 | epoch: 2.7468354430379747
Step 869 | loss: 0.4596414268016815
Step 869 | grad_norm: 1.3777114152908325
Step 869 | learning_rate: 8.438818565400844e-06
Step 869 | epoch: 2.75
Step 870 | loss: 0.565999448299408
Step 870 | grad_norm: 1.3085886240005493
Step 870 | learning_rate: 8.333333333333334e-06
Step 870 | epoch: 2.7531645569620253
Step 871 | loss: 0.42770737409591675
Step 871 | grad_norm: 1.3551256656646729
Step 871 | learning_rate: 8.227848101265822e-06
Step 871 | epoch: 2.7563291139240507
Step 872 | loss: 0.5678255558013916
Step 872 | grad_norm: 1.4965850114822388
Step 872 | learning_rate: 8.122362869198312e-06
Step 872 | epoch: 2.759493670886076
Step 873 | loss: 0.4328024387359619
Step 873 | grad_norm: 1.3579609394073486
Step 873 | learning_rate: 8.016877637130803e-06
Step 873 | epoch: 2.7626582278481013
Step 874 | loss: 0.5129172801971436
Step 874 | grad_norm: 1.787971019744873
Step 874 | learning_rate: 7.911392405063292e-06
Step 874 | epoch: 2.7658227848101267
Step 875 | loss: 0.47890686988830566
Step 875 | grad_norm: 1.2895143032073975
Step 875 | learning_rate: 7.805907172995782e-06
Step 875 | epoch: 2.768987341772152
Step 876 | loss: 0.41069331765174866
Step 876 | grad_norm: 1.2187423706054688
Step 876 | learning_rate: 7.700421940928271e-06
Step 876 | epoch: 2.7721518987341773
Step 877 | loss: 0.590522289276123
Step 877 | grad_norm: 1.3880201578140259
Step 877 | learning_rate: 7.5949367088607605e-06
Step 877 | epoch: 2.7753164556962027
Step 878 | loss: 0.5673041343688965
Step 878 | grad_norm: 1.3275020122528076
Step 878 | learning_rate: 7.4894514767932495e-06
Step 878 | epoch: 2.778481012658228
Step 879 | loss: 0.3866368532180786
Step 879 | grad_norm: 1.4564608335494995
Step 879 | learning_rate: 7.3839662447257386e-06
Step 879 | epoch: 2.7816455696202533
Step 880 | loss: 0.28133344650268555
Step 880 | grad_norm: 1.058984637260437
Step 880 | learning_rate: 7.2784810126582285e-06
Step 880 | epoch: 2.7848101265822782
Step 881 | loss: 0.4635956287384033
Step 881 | grad_norm: 1.539197325706482
Step 881 | learning_rate: 7.1729957805907175e-06
Step 881 | epoch: 2.787974683544304
Step 882 | loss: 0.40208637714385986
Step 882 | grad_norm: 1.164366364479065
Step 882 | learning_rate: 7.0675105485232066e-06
Step 882 | epoch: 2.791139240506329
Step 883 | loss: 0.4229925274848938
Step 883 | grad_norm: 1.013553261756897
Step 883 | learning_rate: 6.9620253164556965e-06
Step 883 | epoch: 2.7943037974683547
Step 884 | loss: 0.461969792842865
Step 884 | grad_norm: 1.5040303468704224
Step 884 | learning_rate: 6.8565400843881855e-06
Step 884 | epoch: 2.7974683544303796
Step 885 | loss: 0.5316201448440552
Step 885 | grad_norm: 1.7358348369598389
Step 885 | learning_rate: 6.7510548523206746e-06
Step 885 | epoch: 2.8006329113924053
Step 886 | loss: 0.4521976709365845
Step 886 | grad_norm: 1.180884838104248
Step 886 | learning_rate: 6.6455696202531645e-06
Step 886 | epoch: 2.8037974683544302
Step 887 | loss: 0.4302184581756592
Step 887 | grad_norm: 1.1885335445404053
Step 887 | learning_rate: 6.540084388185655e-06
Step 887 | epoch: 2.8069620253164556
Step 888 | loss: 0.41334107518196106
Step 888 | grad_norm: 1.2472511529922485
Step 888 | learning_rate: 6.434599156118144e-06
Step 888 | epoch: 2.810126582278481
Step 889 | loss: 0.4282768666744232
Step 889 | grad_norm: 1.4809229373931885
Step 889 | learning_rate: 6.329113924050633e-06
Step 889 | epoch: 2.8132911392405062
Step 890 | loss: 0.3605998754501343
Step 890 | grad_norm: 1.1787939071655273
Step 890 | learning_rate: 6.223628691983123e-06
Step 890 | epoch: 2.8164556962025316
Step 891 | loss: 0.5313922166824341
Step 891 | grad_norm: 1.670812726020813
Step 891 | learning_rate: 6.118143459915612e-06
Step 891 | epoch: 2.819620253164557
Step 892 | loss: 0.43935489654541016
Step 892 | grad_norm: 1.1178877353668213
Step 892 | learning_rate: 6.012658227848101e-06
Step 892 | epoch: 2.8227848101265822
Step 893 | loss: 0.3365882635116577
Step 893 | grad_norm: 1.439208745956421
Step 893 | learning_rate: 5.907172995780591e-06
Step 893 | epoch: 2.8259493670886076
Step 894 | loss: 0.4611290991306305
Step 894 | grad_norm: 1.2810053825378418
Step 894 | learning_rate: 5.80168776371308e-06
Step 894 | epoch: 2.829113924050633
Step 895 | loss: 0.40601059794425964
Step 895 | grad_norm: 1.4302786588668823
Step 895 | learning_rate: 5.69620253164557e-06
Step 895 | epoch: 2.8322784810126582
Step 896 | loss: 0.4537167251110077
Step 896 | grad_norm: 1.2705488204956055
Step 896 | learning_rate: 5.590717299578059e-06
Step 896 | epoch: 2.8354430379746836
Step 897 | loss: 0.43058791756629944
Step 897 | grad_norm: 1.638041615486145
Step 897 | learning_rate: 5.485232067510549e-06
Step 897 | epoch: 2.838607594936709
Step 898 | loss: 0.5133932828903198
Step 898 | grad_norm: 1.473683476448059
Step 898 | learning_rate: 5.379746835443038e-06
Step 898 | epoch: 2.8417721518987342
Step 899 | loss: 0.6242321133613586
Step 899 | grad_norm: 2.0537595748901367
Step 899 | learning_rate: 5.274261603375528e-06
Step 899 | epoch: 2.8449367088607596
Step 900 | loss: 0.4595739543437958
Step 900 | grad_norm: 1.3392006158828735
Step 900 | learning_rate: 5.168776371308017e-06
Step 900 | epoch: 2.848101265822785
Step 901 | loss: 0.4013219475746155
Step 901 | grad_norm: 1.1929208040237427
Step 901 | learning_rate: 5.063291139240506e-06
Step 901 | epoch: 2.8512658227848102
Step 902 | loss: 0.39675474166870117
Step 902 | grad_norm: 1.0965608358383179
Step 902 | learning_rate: 4.957805907172996e-06
Step 902 | epoch: 2.8544303797468356
Step 903 | loss: 0.5478756427764893
Step 903 | grad_norm: 1.2079226970672607
Step 903 | learning_rate: 4.852320675105485e-06
Step 903 | epoch: 2.857594936708861
Step 904 | loss: 0.4673616886138916
Step 904 | grad_norm: 1.3512156009674072
Step 904 | learning_rate: 4.746835443037975e-06
Step 904 | epoch: 2.8607594936708862
Step 905 | loss: 0.44767433404922485
Step 905 | grad_norm: 1.3675709962844849
Step 905 | learning_rate: 4.641350210970465e-06
Step 905 | epoch: 2.8639240506329116
Step 906 | loss: 0.4374202489852905
Step 906 | grad_norm: 1.1815892457962036
Step 906 | learning_rate: 4.535864978902954e-06
Step 906 | epoch: 2.867088607594937
Step 907 | loss: 0.3763068914413452
Step 907 | grad_norm: 1.6889115571975708
Step 907 | learning_rate: 4.430379746835443e-06
Step 907 | epoch: 2.870253164556962
Step 908 | loss: 0.39903825521469116
Step 908 | grad_norm: 1.1455953121185303
Step 908 | learning_rate: 4.324894514767933e-06
Step 908 | epoch: 2.8734177215189876
Step 909 | loss: 0.5037354826927185
Step 909 | grad_norm: 1.2277930974960327
Step 909 | learning_rate: 4.219409282700422e-06
Step 909 | epoch: 2.8765822784810124
Step 910 | loss: 0.4188682436943054
Step 910 | grad_norm: 1.251338005065918
Step 910 | learning_rate: 4.113924050632911e-06
Step 910 | epoch: 2.879746835443038
Step 911 | loss: 0.3843235373497009
Step 911 | grad_norm: 1.2789009809494019
Step 911 | learning_rate: 4.008438818565402e-06
Step 911 | epoch: 2.882911392405063
Step 912 | loss: 0.3062070310115814
Step 912 | grad_norm: 0.9298068881034851
Step 912 | learning_rate: 3.902953586497891e-06
Step 912 | epoch: 2.8860759493670884
Step 913 | loss: 0.3411847949028015
Step 913 | grad_norm: 1.1734497547149658
Step 913 | learning_rate: 3.7974683544303802e-06
Step 913 | epoch: 2.8892405063291138
Step 914 | loss: 0.42226892709732056
Step 914 | grad_norm: 1.450853943824768
Step 914 | learning_rate: 3.6919831223628693e-06
Step 914 | epoch: 2.892405063291139
Step 915 | loss: 0.4415297210216522
Step 915 | grad_norm: 1.3453353643417358
Step 915 | learning_rate: 3.5864978902953588e-06
Step 915 | epoch: 2.8955696202531644
Step 916 | loss: 0.3644173741340637
Step 916 | grad_norm: 1.1401945352554321
Step 916 | learning_rate: 3.4810126582278482e-06
Step 916 | epoch: 2.8987341772151898
Step 917 | loss: 0.44734132289886475
Step 917 | grad_norm: 1.2423298358917236
Step 917 | learning_rate: 3.3755274261603373e-06
Step 917 | epoch: 2.901898734177215
Step 918 | loss: 0.48463618755340576
Step 918 | grad_norm: 1.2640255689620972
Step 918 | learning_rate: 3.2700421940928276e-06
Step 918 | epoch: 2.9050632911392404
Step 919 | loss: 0.47508591413497925
Step 919 | grad_norm: 1.281144142150879
Step 919 | learning_rate: 3.1645569620253167e-06
Step 919 | epoch: 2.9082278481012658
Step 920 | loss: 0.36050745844841003
Step 920 | grad_norm: 1.128876805305481
Step 920 | learning_rate: 3.059071729957806e-06
Step 920 | epoch: 2.911392405063291
Step 921 | loss: 0.3414415717124939
Step 921 | grad_norm: 1.1082019805908203
Step 921 | learning_rate: 2.9535864978902956e-06
Step 921 | epoch: 2.9145569620253164
Step 922 | loss: 0.5227868556976318
Step 922 | grad_norm: 1.3801395893096924
Step 922 | learning_rate: 2.848101265822785e-06
Step 922 | epoch: 2.9177215189873418
Step 923 | loss: 0.5107718706130981
Step 923 | grad_norm: 1.3735874891281128
Step 923 | learning_rate: 2.7426160337552745e-06
Step 923 | epoch: 2.920886075949367
Step 924 | loss: 0.42631837725639343
Step 924 | grad_norm: 1.1660782098770142
Step 924 | learning_rate: 2.637130801687764e-06
Step 924 | epoch: 2.9240506329113924
Step 925 | loss: 0.39235609769821167
Step 925 | grad_norm: 1.4037601947784424
Step 925 | learning_rate: 2.531645569620253e-06
Step 925 | epoch: 2.9272151898734178
Step 926 | loss: 0.5192983746528625
Step 926 | grad_norm: 1.4987661838531494
Step 926 | learning_rate: 2.4261603375527425e-06
Step 926 | epoch: 2.930379746835443
Step 927 | loss: 0.46657824516296387
Step 927 | grad_norm: 1.4996368885040283
Step 927 | learning_rate: 2.3206751054852324e-06
Step 927 | epoch: 2.9335443037974684
Step 928 | loss: 0.36430466175079346
Step 928 | grad_norm: 1.0102059841156006
Step 928 | learning_rate: 2.2151898734177215e-06
Step 928 | epoch: 2.9367088607594938
Step 929 | loss: 0.40178313851356506
Step 929 | grad_norm: 1.228752851486206
Step 929 | learning_rate: 2.109704641350211e-06
Step 929 | epoch: 2.939873417721519
Step 930 | loss: 0.40796518325805664
Step 930 | grad_norm: 1.4079818725585938
Step 930 | learning_rate: 2.004219409282701e-06
Step 930 | epoch: 2.9430379746835444
Step 931 | loss: 0.40170058608055115
Step 931 | grad_norm: 1.2068157196044922
Step 931 | learning_rate: 1.8987341772151901e-06
Step 931 | epoch: 2.9462025316455698
Step 932 | loss: 0.45774754881858826
Step 932 | grad_norm: 1.4195622205734253
Step 932 | learning_rate: 1.7932489451476794e-06
Step 932 | epoch: 2.9493670886075947
Step 933 | loss: 0.48510560393333435
Step 933 | grad_norm: 1.5275242328643799
Step 933 | learning_rate: 1.6877637130801686e-06
Step 933 | epoch: 2.9525316455696204
Step 934 | loss: 0.3860072195529938
Step 934 | grad_norm: 1.225825548171997
Step 934 | learning_rate: 1.5822784810126583e-06
Step 934 | epoch: 2.9556962025316453
Step 935 | loss: 0.34730979800224304
Step 935 | grad_norm: 1.3876487016677856
Step 935 | learning_rate: 1.4767932489451478e-06
Step 935 | epoch: 2.958860759493671
Step 936 | loss: 0.37923675775527954
Step 936 | grad_norm: 1.2370363473892212
Step 936 | learning_rate: 1.3713080168776373e-06
Step 936 | epoch: 2.962025316455696
Step 937 | loss: 0.48181262612342834
Step 937 | grad_norm: 1.377817153930664
Step 937 | learning_rate: 1.2658227848101265e-06
Step 937 | epoch: 2.9651898734177218
Step 938 | loss: 0.3759126663208008
Step 938 | grad_norm: 1.1908100843429565
Step 938 | learning_rate: 1.1603375527426162e-06
Step 938 | epoch: 2.9683544303797467
Step 939 | loss: 0.5625096559524536
Step 939 | grad_norm: 1.6924197673797607
Step 939 | learning_rate: 1.0548523206751055e-06
Step 939 | epoch: 2.971518987341772
Step 940 | loss: 0.486212819814682
Step 940 | grad_norm: 1.3952853679656982
Step 940 | learning_rate: 9.493670886075951e-07
Step 940 | epoch: 2.9746835443037973
Step 941 | loss: 0.4734251797199249
Step 941 | grad_norm: 1.3058435916900635
Step 941 | learning_rate: 8.438818565400843e-07
Step 941 | epoch: 2.9778481012658227
Step 942 | loss: 0.36677882075309753
Step 942 | grad_norm: 1.5042695999145508
Step 942 | learning_rate: 7.383966244725739e-07
Step 942 | epoch: 2.981012658227848
Step 943 | loss: 0.4598744511604309
Step 943 | grad_norm: 1.1796592473983765
Step 943 | learning_rate: 6.329113924050633e-07
Step 943 | epoch: 2.9841772151898733
Step 944 | loss: 0.4149699807167053
Step 944 | grad_norm: 1.3186509609222412
Step 944 | learning_rate: 5.274261603375527e-07
Step 944 | epoch: 2.9873417721518987
Step 945 | loss: 0.3873775899410248
Step 945 | grad_norm: 1.3418132066726685
Step 945 | learning_rate: 4.2194092827004216e-07
Step 945 | epoch: 2.990506329113924
Step 946 | loss: 0.38224172592163086
Step 946 | grad_norm: 1.2149178981781006
Step 946 | learning_rate: 3.1645569620253163e-07
Step 946 | epoch: 2.9936708860759493
Step 947 | loss: 0.3711225688457489
Step 947 | grad_norm: 1.6099743843078613
Step 947 | learning_rate: 2.1097046413502108e-07
Step 947 | epoch: 2.9968354430379747
Step 948 | loss: 0.4316789507865906
Step 948 | grad_norm: 1.612646222114563
Step 948 | learning_rate: 1.0548523206751054e-07
Step 948 | epoch: 3.0
Step 948 | train_runtime: 773.6807
Step 948 | train_samples_per_second: 4.901
Step 948 | train_steps_per_second: 1.225
Step 948 | total_flos: 1.1430927095390208e+16
Step 948 | train_loss: 0.6115988286424286
Step 948 | epoch: 3.0

============================================================
TRAINING COMPLETE
============================================================

LoRA adapter saved.

============================================================
MERGING LORA
============================================================

